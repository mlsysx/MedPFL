{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5236,
     "status": "ok",
     "timestamp": 1680799767553,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "NWa7Xo6PkIl3",
    "outputId": "f9cc715d-5ead-43a8-da77-60c01c2aa530"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "import torchvision\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.nn.functional as func\n",
    "#torch.manual_seed(50)\n",
    "\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "print(torch.__version__, torchvision.__version__)\n",
    "print (torch.cuda.get_device_name(device='cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 848,
     "status": "ok",
     "timestamp": 1680799778241,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "PwMnwVsTLKlP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import random_split\n",
    "from zipfile import ZipFile\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21863,
     "status": "ok",
     "timestamp": 1680799802131,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "3UuKhfxnCJo-",
    "outputId": "7333eeae-7aae-45a8-8557-ca9676e1859b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2301,
     "status": "ok",
     "timestamp": 1680755739957,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "SiMvf-lUCPqX",
    "outputId": "d3eb5622-cb1d-49a2-cb33-3cf622ea2ae6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2867,
     "status": "ok",
     "timestamp": 1680755742822,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "fPz0qakxLSpG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1680755742823,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "TMGVHtjPM-nC"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1680755742823,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "3_hPcjCKNEHq",
    "outputId": "991bca65-c3a0-44ac-ccc9-63414083914e"
   },
   "outputs": [],
   "source": [
    "data_dir = 'Covid_chest_X-raydata\\Covid_chest_X-raydata\\Covid19-dataset'\n",
    "\n",
    "print(os.listdir(data_dir))\n",
    "classes = os.listdir(data_dir + \"/train\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1680755743145,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "Eco6nQxQNEKj"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1680755743145,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "yMrlSX0qNENI"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     #transforms.CenterCrop(32),\n",
    "     transforms.Resize((32,32)),\n",
    "     #transforms.RandomCrop(32),\n",
    "     #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "     #transforms.ConvertImageDtype(torch.float)\n",
    "     ])\n",
    "\n",
    "dataset = ImageFolder(data_dir+'/train', transform=transform)\n",
    "\n",
    "#dataset1 = DataLoader(trainset, shuffle=True, batch_size=batch_size, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21763,
     "status": "ok",
     "timestamp": 1680755764906,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "lkggrlnANEOi"
   },
   "outputs": [],
   "source": [
    "image_data=[]\n",
    "target=[]\n",
    "for i, j in dataset:\n",
    "  image_data.append(i)\n",
    "  target.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1680755764907,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "Y_96R6a1cpRS"
   },
   "outputs": [],
   "source": [
    "#image_dataaaa=image_data[0:50]\n",
    "#targetttt=target[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1680755764908,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "EBeG7N3jNERf",
    "outputId": "84952d7a-c2fd-4cb3-827e-8dacaba02afa"
   },
   "outputs": [],
   "source": [
    "image_data1=torch.stack(image_data)\n",
    "image_data2=image_data1.numpy()\n",
    "image_data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1680755764908,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "LmEISz87OkMB",
    "outputId": "568839f2-4e13-42bf-de9c-c6183e075819"
   },
   "outputs": [],
   "source": [
    "qq=np.rollaxis(image_data2,1,4)\n",
    "qq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1680755764909,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "SmMkra5_OkO0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1680755764909,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "IPmdIIPwOkRw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1680755764910,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "byjt1exCOkUV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1680755764910,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "qhFjqpy0OkXx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1680755764911,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "ojd1oReP20Gq",
    "outputId": "064d830d-2517-41b6-ed4e-9c5f52b533e5"
   },
   "outputs": [],
   "source": [
    "# dst = datasets.CIFAR100(\"~/.torch\", download=True)\n",
    "# dst = datasets.MNIST(\"~/.torch\", download=True)\n",
    "\n",
    "tp = transforms.Compose([\n",
    "    #transforms.Resize(32),\n",
    "    #transforms.CenterCrop(32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "tt = transforms.ToPILImage()\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "print(\"Running on %s\" % device)\n",
    "\n",
    "def label_to_onehot(target, num_classes=3):\n",
    "    target = torch.unsqueeze(target, 1)\n",
    "    onehot_target = torch.zeros(target.size(0), num_classes, device=target.device)\n",
    "    onehot_target.scatter_(1, target, 1)\n",
    "    return onehot_target\n",
    "\n",
    "def cross_entropy_for_onehot(pred, target):\n",
    "    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1680755764911,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "AorI020iVjjS"
   },
   "outputs": [],
   "source": [
    "# def weights_init(m):\n",
    "#     if hasattr(m, \"weight\"):\n",
    "#         m.weight.data.uniform_(-0.5, 0.5)\n",
    "#         nn.init.xavier_uniform_(m.weight.data)\n",
    "#     if hasattr(m, \"bias\"):\n",
    "#         #m.bias.data.uniform_(-0.5, 0.5)\n",
    "#         #nn.init.xavier_uniform(m.bias.data)\n",
    "#         m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "\n",
    "# class LeNet(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "\n",
    "#         super(LeNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, kernel_size=5,stride=2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=2)\n",
    "#         self.fc1 = nn.Linear(16*5*5, 256)\n",
    "#         self.fc2 = nn.Linear(256, 120)\n",
    "#         self.fc3 = nn.Linear(120, 106)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         #x = func.relu(self.conv1(x))\n",
    "#         x = func.sigmoid(self.conv1(x))\n",
    "#         #x = func.max_pool2d(x, 2)\n",
    "#         #x = func.relu(self.conv2(x))\n",
    "#         x = func.sigmoid(self.conv2(x))\n",
    "#         #x = func.max_pool2d(x, 2)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         #x = func.relu(self.fc1(x))\n",
    "#         x = func.sigmoid(self.fc1(x))\n",
    "#         #x = func.relu(self.fc2(x))\n",
    "#         x = func.sigmoid(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "    \n",
    "    \n",
    "# def weights_init(m):\n",
    "#     if hasattr(m, \"weight\"):\n",
    "#         m.weight.data.uniform_(-0.3, 0.3)\n",
    "#     if hasattr(m, \"bias\"):\n",
    "#         m.bias.data.uniform_(-0.3, 0.3)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if hasattr(m, \"weight\"):\n",
    "        m.weight.data.uniform_(-0.5, 0.5)\n",
    "    if hasattr(m, \"bias\"):\n",
    "        m.bias.data.uniform_(-0.5, 0.5)\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        act = nn.Sigmoid\n",
    "        #act = nn.ReLU\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, kernel_size=5, padding=5//2, stride=2),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=2),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=1),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=1),\n",
    "            act(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # print(out.size())\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "net = LeNet().to(device)\n",
    "net.apply(weights_init)\n",
    "\n",
    "\n",
    "\n",
    "#criterion = cross_entropy_for_onehot\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1680756659159,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "lxPnL3k220Gs",
    "outputId": "3c95eea1-7a40-4b15-baf4-518ccf3716dc"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "#lfw_people=fetch_lfw_people(min_faces_per_person=14,color=True,slice_=(slice(61,189),slice(61,189)),resize=0.25) #14\n",
    "#x=lfw_people.images\n",
    "#y=lfw_people.target\n",
    "x=qq\n",
    "y=target\n",
    "\n",
    "#target_names=lfw_people.target_names\n",
    "n_classes=3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "print (X_train.shape)\n",
    "print (X_test.shape)\n",
    "print (n_classes)\n",
    "\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1680756660709,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "ZcUt0dpiDYaV"
   },
   "outputs": [],
   "source": [
    "test_size=len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1680756662369,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "v-Sc-cWL20Gt",
    "outputId": "cd5f9e70-08d7-4fa1-f887-363ab67aa4b6"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "#X_train = torch.transpose\n",
    "#X_train = X_train.astype('float32')\n",
    "#X_train /= 255.0\n",
    "#X_test /= 255.0\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "x_train = torch.FloatTensor(X_train).to(device)\n",
    "x_train = x_train.transpose(2,3).transpose(1,2)\n",
    "y_train = torch.LongTensor(y_train).to(device)\n",
    "\n",
    "x_test = torch.FloatTensor(X_test).to(device)\n",
    "x_test = x_test.transpose(2,3).transpose(1,2)\n",
    "y_test = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "\n",
    "training = data.TensorDataset(x_train,y_train)\n",
    "\n",
    "testing = data.TensorDataset(x_test,y_test)\n",
    "\n",
    "dst_tensor=training\n",
    "\n",
    "criterion_train = nn.CrossEntropyLoss()\n",
    "optimizer_train = optim.Adam(net.parameters(),lr=0.01)#,momentum=0.9)\n",
    "trainloader = torch.utils.data.DataLoader(training, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(0):\n",
    "\n",
    "    for i,data in enumerate(trainloader,0):\n",
    "    #for data in trainloader:\n",
    "        #if i<=10: \n",
    "\n",
    "            inputs,label = data\n",
    "\n",
    "            inputs,label =  Variable(inputs).to(device),Variable(label).to(device)\n",
    "\n",
    "            optimizer_train.zero_grad()\n",
    "            outputs_benign=net(inputs)\n",
    "\n",
    "            loss_benign =  criterion_train(outputs_benign,label)\n",
    "\n",
    "            loss_benign.backward()\n",
    "            #sgd_update(net.parameters())\n",
    "\n",
    "            optimizer_train.step()\n",
    "            \n",
    "            testloader = torch.utils.data.DataLoader(testing,batch_size=test_size, shuffle=False)\n",
    "\n",
    "            acc =0.0\n",
    "            for ji,tdata in enumerate(testloader,0):\n",
    "\n",
    "                tinputs,tlabel = tdata\n",
    "\n",
    "                tinputs,tlabel =  Variable(tinputs).to(device),Variable(tlabel).to(device)\n",
    "\n",
    "                toutputs=net(tinputs)\n",
    "\n",
    "                tpredict = torch.argmax(toutputs, dim=1)\n",
    "\n",
    "\n",
    "                for mi in range(test_size):\n",
    "\n",
    "\n",
    "\n",
    "                    if tpredict[mi] == tlabel[mi]:\n",
    "                        acc=acc+1\n",
    "\n",
    "            accuracy = acc / test_size\n",
    "            print (accuracy)\n",
    "            print ('fininshed testing')\n",
    "            if accuracy>0.18:\n",
    "                break\n",
    "\n",
    "\n",
    "print ('fininshed training')\n",
    "#torch.save(net.state_dict()\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testing,batch_size=test_size, shuffle=False)\n",
    "\n",
    "acc =0.0\n",
    "for ji,tdata in enumerate(testloader,0):\n",
    "\n",
    "    tinputs,tlabel = tdata\n",
    "\n",
    "    tinputs,tlabel =  Variable(tinputs).to(device),Variable(tlabel).to(device)\n",
    "\n",
    "    toutputs=net(tinputs)\n",
    "    \n",
    "    tpredict = torch.argmax(toutputs, dim=1)\n",
    "    \n",
    "   \n",
    "    for mi in range(test_size):\n",
    "        \n",
    "        \n",
    "\n",
    "        if tpredict[mi] == tlabel[mi]:\n",
    "            acc=acc+1\n",
    "\n",
    "accuracy = acc / test_size\n",
    "print (accuracy)\n",
    "print ('fininshed testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1680756667054,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "uiXtirStEIi3",
    "outputId": "a2368a1f-179f-4721-ae66-77a2b29150b9"
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train[30], alpha =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1911,
     "status": "ok",
     "timestamp": 1680755767222,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "2xZZM5YBV259",
    "outputId": "5e2426b8-ceaa-4d1f-f1d7-7dfe1f2ad239"
   },
   "outputs": [],
   "source": [
    "pip install pytorch-msssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "######### honest partipant #########\n",
    "img_index = 30   #use img_index\n",
    "dst_pil = tt(dst_tensor[img_index][0].cpu())   #use img_index\n",
    "\n",
    "gt_data = tp(dst_pil).to(device)\n",
    "gt_data = torch.unsqueeze(gt_data,0)\n",
    "\n",
    "gt_label = dst_tensor[img_index][1].long().to(device) #use img_index\n",
    "gt_label = gt_label.view(1, )\n",
    "gt_onehot_label = label_to_onehot(gt_label, num_classes=3)\n",
    "\n",
    "plt.imshow(dst_pil)\n",
    "#plt.savefig(\"./original/index_%s_label_%s\"%(img_index,gt_label.item()))\n",
    "\n",
    "\n",
    "\n",
    "batch =1  #\n",
    "for bat in range(batch-1):\n",
    "    dst_pil = tt(dst_tensor[img_index+1+bat][0].cpu())   #use img_index\n",
    "    tmp = torch.unsqueeze(tp(dst_pil).to(device),0)\n",
    "    #print(tmp.shape)\n",
    "    gt_data = torch.cat((gt_data,tmp),0)\n",
    "\n",
    "    gt_label_tmp = dst_tensor[img_index+1+bat][1].long().to(device) #use img_index\n",
    "    gt_label_tmp = gt_label_tmp.view(1, )\n",
    "    gt_label = torch.cat((gt_label,gt_label_tmp),0)\n",
    "    gt_onehot_label = torch.cat((gt_onehot_label,label_to_onehot(gt_label_tmp, num_classes=3)),0)\n",
    "\n",
    "    if gt_label_tmp ==60:\n",
    "        print (bat)\n",
    "\n",
    "    plt.imshow(dst_pil)\n",
    "    #plt.savefig(\"./original/index_%s_label_%s\"%(bat+1,gt_label_tmp.item()))\n",
    "\n",
    "    #plt.title(\"Ground truth image\")\n",
    "    #print(\"GT label is %d.\" % gt_label.item(), \"\\nOnehot label is %d.\" % torch.argmax(gt_onehot_label, dim=-1).item())\n",
    "\n",
    "\n",
    "gt_label = torch.reshape(gt_label,(-1,1))    \n",
    "print (gt_data.shape)\n",
    "print (gt_label.shape)\n",
    "print (gt_label)\n",
    "print (gt_onehot_label.shape)\n",
    "\n",
    "plt.imshow(tt(gt_data[0].cpu()),cmap='gray')\n",
    "# plt.axis('off')\n",
    "# plt.savefig(\"./attack_image/tifs\")\n",
    "\n",
    "\n",
    "# compute original gradient \n",
    "dy_dx = []\n",
    "original_dy_dx=[]\n",
    "original_pred = []\n",
    "for item in range(batch):\n",
    "    gt_data_single = torch.unsqueeze(gt_data[item],0)\n",
    "    out = net(gt_data_single)\n",
    "    #y = criterion(out, gt_onehot_label[item])\n",
    "    y = criterion(out, gt_label[item])\n",
    "    dy_dx = torch.autograd.grad(y, net.parameters(),retain_graph=True)\n",
    "    original_dy_dx_tmp = list((_.detach().clone() for _ in dy_dx))\n",
    "    original_dy_dx.append(original_dy_dx_tmp)\n",
    "    out_tmp = out.detach().clone()\n",
    "    original_pred.append(out_tmp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #FOR fully-connected model only\n",
    "#     dw = net.body[0].weight\n",
    "#     db = net.body[0].bias\n",
    "#     dy_dw = torch.autograd.grad(y, dw,retain_graph=True)\n",
    "#     dy_db = torch.autograd.grad(y, db,retain_graph=True)\n",
    "\n",
    "#     print (dy_dw)\n",
    "#     #print (dy_db.shape)\n",
    "\n",
    "#     leak=dy_dw/dy_db\n",
    "\n",
    "#     print (leak.shape)\n",
    "\n",
    "\n",
    "\n",
    "# share the gradients with other clients\n",
    "#original_dy_dx = list((_.detach().clone() for _ in dy_dx))\n",
    "\n",
    "\n",
    "# generate dummy data and label\n",
    "\n",
    "import time\n",
    "\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "\n",
    "#print (ssim(0.43*torch.unsqueeze(gt_data[0],dim=0),torch.unsqueeze(gt_data[0],dim=0),data_range=0).item())\n",
    "#print (torch.dist(0.6*torch.unsqueeze(gt_data[0],dim=0),torch.unsqueeze(gt_data[0],dim=0),2).item())\n",
    "\n",
    "\n",
    "for item in range(1):\n",
    "    start = time.process_time()\n",
    "    for rd in range(1):\n",
    "\n",
    "        torch.manual_seed(200*rd)\n",
    "        #dummy_data = torch.unsqueeze(torch.randn(gt_data[item].size()),0).to(device).requires_grad_(True)\n",
    "\n",
    "        #dummy_data = torch.unsqueeze(torch.zeros(gt_data[item].size()),0).to(device).requires_grad_(True)\n",
    "        #dummy_data = torch.unsqueeze(torch.ones(gt_data[item].size()),0).to(device).requires_grad_(True)\n",
    "\n",
    "\n",
    "        #background = torch.unsqueeze(torch.zeros(gt_data[item].size()),0)\n",
    "        #background[0,0,::] = 1\n",
    "        #dummy_data = background.to(device).requires_grad_(True)\n",
    "        ##dummy_data = (torch.unsqueeze(torch.randn(gt_data[item].size()),0)+background).to(device).requires_grad_(True)\n",
    "\n",
    "        #surrogate = torch.unsqueeze(gt_data[item+1],0)\n",
    "        #aaa = torch.rand([3,16,16])\n",
    "        #surrogate[0,:,8:24,8:24] =aaa\n",
    "        #dummy_data = surrogate.to(device).requires_grad_(True)    \n",
    "\n",
    "        #dummy_data = torch.unsqueeze(gt_data[item+1],0).to(device).requires_grad_(True)\n",
    "\n",
    "        #k = np.random.randint(0,95)\n",
    "        #dummy_data = torch.unsqueeze(gt_data[k],0).to(device).requires_grad_(True)\n",
    "\n",
    "\n",
    "        pat_1 = torch.rand([3,16,16])\n",
    "        pat_2 = torch.cat((pat_1,pat_1),dim=1)\n",
    "        pat_4 = torch.cat((pat_2,pat_2),dim=2)\n",
    "        dummy_data = torch.unsqueeze(pat_4,dim=0).to(device).requires_grad_(True)\n",
    "\n",
    "\n",
    "        #aaa = torch.rand([3,8,8])\n",
    "        #bbb = torch.cat((aaa,aaa),dim=1)\n",
    "        #ccc = torch.cat((bbb,bbb),dim=1)\n",
    "        #ddd = torch.cat((ccc,ccc),dim=2)\n",
    "        #eee = torch.cat((ddd,ddd),dim=2)\n",
    "        #dummy_data = torch.unsqueeze(eee,dim=0).to(device).requires_grad_(True)\n",
    "\n",
    "        #aaa = torch.rand([3,4,4])\n",
    "        #bbb = torch.cat((aaa,aaa),dim=1)\n",
    "        #ccc = torch.cat((bbb,bbb),dim=1)\n",
    "        #ddd = torch.cat((ccc,ccc),dim=1)\n",
    "        #eee = torch.cat((ddd,ddd),dim=2)\n",
    "        #fff = torch.cat((eee,eee),dim=2)\n",
    "        #ggg = torch.cat((fff,fff),dim=2)\n",
    "        #dummy_data = torch.unsqueeze(ggg,dim=0).to(device).requires_grad_(True)\n",
    "\n",
    "\n",
    "        #dummy_data = plt.imread(\"./attack_image/replacement_69.png\")\n",
    "        #print (dummy_data.shape)\n",
    "        #dummy_data = torch.FloatTensor(dummy_data).to(device)\n",
    "        #dummy_data = dummy_data.transpose(2,3).transpose(1,2)\n",
    "\n",
    "        dummy_unsqueeze=torch.unsqueeze(gt_onehot_label[item],dim=0)\n",
    "\n",
    "        dummy_label = torch.randn(dummy_unsqueeze.size()).to(device).requires_grad_(True)\n",
    "        label_pred=torch.argmin(torch.sum(original_dy_dx[item][-2], dim=-1), \n",
    "                                dim=-1).detach().reshape((1,)).requires_grad_(False)\n",
    "        #print (original_dy_dx[item][-1].shape)\n",
    "        #print (original_dy_dx[item][-1].argmin())\n",
    "\n",
    "        #print (torch.sum(original_dy_dx[item][-2], dim=-1).argmin())\n",
    "\n",
    "        plt.imshow(tt(dummy_data[0].cpu()))\n",
    "        plt.title(\"Dummy data\")\n",
    "        #plt.savefig(\"./random_seed/index_%s_rand_seed_%s_label_%s\"%(item,rd,torch.argmax(dummy_label, dim=-1).item()))\n",
    "\n",
    "        plt.clf()\n",
    "        print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "        print(\"stolen label is %d.\" % label_pred.item())\n",
    "\n",
    "\n",
    "        #optimizer = torch.optim.LBFGS([dummy_data,dummy_label])\n",
    "        optimizer = torch.optim.LBFGS([dummy_data,])\n",
    "        #optimizer = torch.optim.AdamW([dummy_data,],lr=0.01)\n",
    "        #optimizer = torch.optim.SGD([dummy_data,],lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "        history = []\n",
    "\n",
    "        percept_dis = np.zeros(300)\n",
    "        recover_dis = np.zeros(300)\n",
    "        for iters in range(300):\n",
    "\n",
    "\n",
    "            percept_dis[iters]=ssim(dummy_data,torch.unsqueeze(gt_data[item],dim=0),data_range=0).item()\n",
    "            #recover_dis[iters]=torch.dist(dummy_data,torch.unsqueeze(gt_data[item],dim=0),2).item()\n",
    "            recover_dis[iters]= F.mse_loss(dummy_data,torch.unsqueeze(gt_data[item],dim=0)).item()\n",
    "\n",
    "            history.append(tt(dummy_data[0].cpu()))\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pred = net(dummy_data) \n",
    "\n",
    "                #dummy_onehot_label = F.softmax(dummy_label, dim=-1).long()\n",
    "\n",
    "                #dummy_loss = criterion(pred, dummy_onehot_label) # TODO: fix the gt_label to dummy_label in both code and slides.\n",
    "                ##print (pred)\n",
    "                ##print (label_pred)\n",
    "\n",
    "                dummy_loss = criterion(pred, label_pred)\n",
    "                dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "                ##dummy_dy_dp = torch.autograd.grad(dummy_loss, dummy_data, create_graph=True)\n",
    "                ##print (dummy_dy_dp[0].shape)  \n",
    "\n",
    "                grad_diff = 0\n",
    "                grad_count = 0\n",
    "                #count =0\n",
    "                for gx, gy in zip(dummy_dy_dx, original_dy_dx[item]): # TODO: fix the variablas here\n",
    "\n",
    "                    #if iters==500 or iters== 1200:\n",
    "                    #print (gx[0])\n",
    "                    #    print ('hahaha')\n",
    "                    #print (gy[0])\n",
    "                    lasso = torch.norm(dummy_data,p=1)\n",
    "                    ridge = torch.norm(dummy_data,p=2)\n",
    "                    grad_diff += ((gx - gy) ** 2).sum() #+ 0.0*lasso +0.01*ridge \n",
    "\n",
    "                    #print (gx.shape)\n",
    "\n",
    "                    grad_count += gx.nelement()\n",
    "\n",
    "\n",
    "                    #if count == 9:\n",
    "                    #    break\n",
    "                    #count=count+1\n",
    "                # grad_diff = grad_diff / grad_count * 1000\n",
    "\n",
    "                #grad_diff += ((original_pred[item]-pred)**2).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                grad_diff.backward()\n",
    "                #print (count)\n",
    "\n",
    "                #print (dummy_dy_dx)\n",
    "                #print (original_dy_dx)\n",
    "\n",
    "\n",
    "                return grad_diff\n",
    "\n",
    "\n",
    "\n",
    "            optimizer.step(closure)\n",
    "            if iters % 5 == 0: \n",
    "                current_loss = closure()\n",
    "                #if iters == 0: \n",
    "                print (\"%.8f\" % current_loss.item())\n",
    "                #print(iters, \"%.8f\" % current_loss.item())\n",
    "            history.append(tt(dummy_data[0].cpu()))\n",
    "\n",
    "\n",
    "\n",
    "        #plt.figure(figsize=(18, 12))\n",
    "        #for i in range(60):\n",
    "        #  plt.subplot(6, 10, i + 1)\n",
    "        #  plt.imshow(history[i * 5])\n",
    "        #  plt.title(\"iter=%d\" % (i * 5))\n",
    "        #  plt.axis('off')\n",
    "\n",
    "        plt.figure(figsize=(12, 1.5))\n",
    "        #iter_idx = [0,20,40,60,80,100,120,140,160,180]\n",
    "        plt.figure(figsize=(6.5, 1.2))\n",
    "        #iter_idx = [0,1000,2000,3000,4000,5000]\n",
    "        iter_idx = [0,5,10,20,50,100]\n",
    "\n",
    "\n",
    "        for i in range(6):\n",
    "          plt.subplot(1, 6, i + 1)\n",
    "          plt.imshow(history[iter_idx[i]])\n",
    "          plt.title(\"iter=%d\" % (iter_idx[i]))\n",
    "          plt.axis('off')\n",
    "\n",
    "        #np.savetxt('ssim_random2',percept_dis,fmt=\"%4f\")\n",
    "        #np.savetxt('mse_random2',recover_dis,fmt=\"%4f\")\n",
    "\n",
    "        #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "        #plt.savefig(\"./attack_image/index_%s_rand_%s_label_%s\"%(item,rd, label_pred.item()))\n",
    "        #plt.clf()\n",
    "\n",
    "    duration = time.process_time()-start\n",
    "    #print (\"Running time is %.4f.\" %(duration/10.0) )\n",
    "    print (duration/10.0 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gau_rate_list = [100]\n",
    "for gau_rate in gau_rate_list:\n",
    "    print('now starting',gau_rate)\n",
    "\n",
    "\n",
    "    ######### honest partipant #########\n",
    "    img_index = 30   #use img_index\n",
    "    dst_pil = tt(dst_tensor[img_index][0].cpu())   #use img_index\n",
    "\n",
    "    gt_data = tp(dst_pil).to(device)\n",
    "    gt_data = torch.unsqueeze(gt_data,0)\n",
    "\n",
    "    gt_label = dst_tensor[img_index][1].long().to(device) #use img_index\n",
    "    gt_label = gt_label.view(1, )\n",
    "    gt_onehot_label = label_to_onehot(gt_label, num_classes=3)\n",
    "\n",
    "    plt.imshow(dst_pil)\n",
    "    #plt.savefig(\"./original/index_%s_label_%s\"%(img_index,gt_label.item()))\n",
    "\n",
    "\n",
    "\n",
    "    batch =1  #\n",
    "    for bat in range(batch-1):\n",
    "        dst_pil = tt(dst_tensor[img_index+1+bat][0].cpu())   #use img_index\n",
    "        tmp = torch.unsqueeze(tp(dst_pil).to(device),0)\n",
    "        #print(tmp.shape)\n",
    "        gt_data = torch.cat((gt_data,tmp),0)\n",
    "\n",
    "        gt_label_tmp = dst_tensor[img_index+1+bat][1].long().to(device) #use img_index\n",
    "        gt_label_tmp = gt_label_tmp.view(1, )\n",
    "        gt_label = torch.cat((gt_label,gt_label_tmp),0)\n",
    "        gt_onehot_label = torch.cat((gt_onehot_label,label_to_onehot(gt_label_tmp, num_classes=3)),0)\n",
    "\n",
    "        if gt_label_tmp ==60:\n",
    "            print (bat)\n",
    "\n",
    "        plt.imshow(dst_pil)\n",
    "        #plt.savefig(\"./original/index_%s_label_%s\"%(bat+1,gt_label_tmp.item()))\n",
    "\n",
    "        #plt.title(\"Ground truth image\")\n",
    "        #print(\"GT label is %d.\" % gt_label.item(), \"\\nOnehot label is %d.\" % torch.argmax(gt_onehot_label, dim=-1).item())\n",
    "\n",
    "\n",
    "    gt_label = torch.reshape(gt_label,(-1,1))    \n",
    "    print (gt_data.shape)\n",
    "    print (gt_label.shape)\n",
    "    print (gt_label)\n",
    "    print (gt_onehot_label.shape)\n",
    "\n",
    "    plt.imshow(tt(gt_data[0].cpu()),cmap='gray')\n",
    "    # plt.axis('off')\n",
    "    # plt.savefig(\"./attack_image/tifs\")\n",
    "\n",
    "\n",
    "    # compute original gradient \n",
    "    dy_dx = []\n",
    "    original_dy_dx=[]\n",
    "    original_pred = []\n",
    "    for item in range(batch):\n",
    "        gt_data_single = torch.unsqueeze(gt_data[item],0)\n",
    "        out = net(gt_data_single)\n",
    "        #y = criterion(out, gt_onehot_label[item])\n",
    "        y = criterion(out, gt_label[item])\n",
    "        dy_dx = torch.autograd.grad(y, net.parameters(),retain_graph=True)\n",
    "        original_dy_dx_tmp = list((_.detach().clone() for _ in dy_dx))\n",
    "        original_dy_dx.append(original_dy_dx_tmp)\n",
    "        out_tmp = out.detach().clone()\n",
    "        original_pred.append(out_tmp)\n",
    "\n",
    "        \n",
    "    #if gaussian noise or laplace\n",
    "        m = torch.distributions.laplace.Laplace(torch.tensor([0.0]), torch.tensor([1/gau_rate]))\n",
    "        for item in range(batch):\n",
    "            for layer_idx in range(10):\n",
    "                #original_dy_dx[item][layer_idx] =  original_dy_dx[item][layer_idx] + torch.empty(original_dy_dx[item][layer_idx].size()).normal_(mean=0,std=1/gau_rate).to(device)\n",
    "                original_dy_dx[item][layer_idx] =  original_dy_dx[item][layer_idx] + torch.squeeze(m.sample(sample_shape=original_dy_dx[item][layer_idx].size()),dim=-1).to(device)\n",
    "        #dy_dx.append(torch.autograd.grad(y, net.parameters()))\n",
    "\n",
    "\n",
    "\n",
    "    # #FOR fully-connected model only\n",
    "    #     dw = net.body[0].weight\n",
    "    #     db = net.body[0].bias\n",
    "    #     dy_dw = torch.autograd.grad(y, dw,retain_graph=True)\n",
    "    #     dy_db = torch.autograd.grad(y, db,retain_graph=True)\n",
    "\n",
    "    #     print (dy_dw)\n",
    "    #     #print (dy_db.shape)\n",
    "\n",
    "    #     leak=dy_dw/dy_db\n",
    "\n",
    "    #     print (leak.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # share the gradients with other clients\n",
    "    #original_dy_dx = list((_.detach().clone() for _ in dy_dx))\n",
    "\n",
    "\n",
    "    # generate dummy data and label\n",
    "\n",
    "    import time\n",
    "\n",
    "    from pytorch_msssim import ssim\n",
    "\n",
    "\n",
    "    #print (ssim(0.43*torch.unsqueeze(gt_data[0],dim=0),torch.unsqueeze(gt_data[0],dim=0),data_range=0).item())\n",
    "    #print (torch.dist(0.6*torch.unsqueeze(gt_data[0],dim=0),torch.unsqueeze(gt_data[0],dim=0),2).item())\n",
    "\n",
    "\n",
    "    for item in range(1):\n",
    "        start = time.process_time()\n",
    "        for rd in range(1):\n",
    "\n",
    "            torch.manual_seed(200*rd)\n",
    "            #dummy_data = torch.unsqueeze(torch.randn(gt_data[item].size()),0).to(device).requires_grad_(True)\n",
    "\n",
    "            #dummy_data = torch.unsqueeze(torch.zeros(gt_data[item].size()),0).to(device).requires_grad_(True)\n",
    "            #dummy_data = torch.unsqueeze(torch.ones(gt_data[item].size()),0).to(device).requires_grad_(True)\n",
    "\n",
    "\n",
    "            #background = torch.unsqueeze(torch.zeros(gt_data[item].size()),0)\n",
    "            #background[0,0,::] = 1\n",
    "            #dummy_data = background.to(device).requires_grad_(True)\n",
    "            ##dummy_data = (torch.unsqueeze(torch.randn(gt_data[item].size()),0)+background).to(device).requires_grad_(True)\n",
    "\n",
    "            #surrogate = torch.unsqueeze(gt_data[item+1],0)\n",
    "            #aaa = torch.rand([3,16,16])\n",
    "            #surrogate[0,:,8:24,8:24] =aaa\n",
    "            #dummy_data = surrogate.to(device).requires_grad_(True)    \n",
    "\n",
    "            #dummy_data = torch.unsqueeze(gt_data[item+1],0).to(device).requires_grad_(True)\n",
    "\n",
    "            #k = np.random.randint(0,95)\n",
    "            #dummy_data = torch.unsqueeze(gt_data[k],0).to(device).requires_grad_(True)\n",
    "\n",
    "\n",
    "            pat_1 = torch.rand([3,16,16])\n",
    "            pat_2 = torch.cat((pat_1,pat_1),dim=1)\n",
    "            pat_4 = torch.cat((pat_2,pat_2),dim=2)\n",
    "            dummy_data = torch.unsqueeze(pat_4,dim=0).to(device).requires_grad_(True)\n",
    "\n",
    "\n",
    "            #aaa = torch.rand([3,8,8])\n",
    "            #bbb = torch.cat((aaa,aaa),dim=1)\n",
    "            #ccc = torch.cat((bbb,bbb),dim=1)\n",
    "            #ddd = torch.cat((ccc,ccc),dim=2)\n",
    "            #eee = torch.cat((ddd,ddd),dim=2)\n",
    "            #dummy_data = torch.unsqueeze(eee,dim=0).to(device).requires_grad_(True)\n",
    "\n",
    "            #aaa = torch.rand([3,4,4])\n",
    "            #bbb = torch.cat((aaa,aaa),dim=1)\n",
    "            #ccc = torch.cat((bbb,bbb),dim=1)\n",
    "            #ddd = torch.cat((ccc,ccc),dim=1)\n",
    "            #eee = torch.cat((ddd,ddd),dim=2)\n",
    "            #fff = torch.cat((eee,eee),dim=2)\n",
    "            #ggg = torch.cat((fff,fff),dim=2)\n",
    "            #dummy_data = torch.unsqueeze(ggg,dim=0).to(device).requires_grad_(True)\n",
    "\n",
    "\n",
    "            #dummy_data = plt.imread(\"./attack_image/replacement_69.png\")\n",
    "            #print (dummy_data.shape)\n",
    "            #dummy_data = torch.FloatTensor(dummy_data).to(device)\n",
    "            #dummy_data = dummy_data.transpose(2,3).transpose(1,2)\n",
    "\n",
    "            dummy_unsqueeze=torch.unsqueeze(gt_onehot_label[item],dim=0)\n",
    "\n",
    "            dummy_label = torch.randn(dummy_unsqueeze.size()).to(device).requires_grad_(True)\n",
    "            label_pred=torch.argmin(torch.sum(original_dy_dx[item][-2], dim=-1), \n",
    "                                    dim=-1).detach().reshape((1,)).requires_grad_(False)\n",
    "            #print (original_dy_dx[item][-1].shape)\n",
    "            #print (original_dy_dx[item][-1].argmin())\n",
    "\n",
    "            #print (torch.sum(original_dy_dx[item][-2], dim=-1).argmin())\n",
    "\n",
    "            plt.imshow(tt(dummy_data[0].cpu()))\n",
    "            plt.title(\"Dummy data\")\n",
    "            #plt.savefig(\"./random_seed/index_%s_rand_seed_%s_label_%s\"%(item,rd,torch.argmax(dummy_label, dim=-1).item()))\n",
    "\n",
    "            plt.clf()\n",
    "            print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "            print(\"stolen label is %d.\" % label_pred.item())\n",
    "\n",
    "\n",
    "            #optimizer = torch.optim.LBFGS([dummy_data,dummy_label])\n",
    "            optimizer = torch.optim.LBFGS([dummy_data,])\n",
    "            #optimizer = torch.optim.AdamW([dummy_data,],lr=0.01)\n",
    "            #optimizer = torch.optim.SGD([dummy_data,],lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "            history = []\n",
    "\n",
    "            percept_dis = np.zeros(300)\n",
    "            recover_dis = np.zeros(300)\n",
    "            for iters in range(300):\n",
    "\n",
    "\n",
    "                percept_dis[iters]=ssim(dummy_data,torch.unsqueeze(gt_data[item],dim=0),data_range=0).item()\n",
    "                #recover_dis[iters]=torch.dist(dummy_data,torch.unsqueeze(gt_data[item],dim=0),2).item()\n",
    "                recover_dis[iters]= F.mse_loss(dummy_data,torch.unsqueeze(gt_data[item],dim=0)).item()\n",
    "\n",
    "                history.append(tt(dummy_data[0].cpu()))\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    pred = net(dummy_data) \n",
    "\n",
    "                    #dummy_onehot_label = F.softmax(dummy_label, dim=-1).long()\n",
    "\n",
    "                    #dummy_loss = criterion(pred, dummy_onehot_label) # TODO: fix the gt_label to dummy_label in both code and slides.\n",
    "                    ##print (pred)\n",
    "                    ##print (label_pred)\n",
    "\n",
    "                    dummy_loss = criterion(pred, label_pred)\n",
    "                    dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "                    ##dummy_dy_dp = torch.autograd.grad(dummy_loss, dummy_data, create_graph=True)\n",
    "                    ##print (dummy_dy_dp[0].shape)  \n",
    "\n",
    "                    grad_diff = 0\n",
    "                    grad_count = 0\n",
    "                    #count =0\n",
    "                    for gx, gy in zip(dummy_dy_dx, original_dy_dx[item]): # TODO: fix the variablas here\n",
    "\n",
    "                        #if iters==500 or iters== 1200:\n",
    "                        #print (gx[0])\n",
    "                        #    print ('hahaha')\n",
    "                        #print (gy[0])\n",
    "                        lasso = torch.norm(dummy_data,p=1)\n",
    "                        ridge = torch.norm(dummy_data,p=2)\n",
    "                        grad_diff += ((gx - gy) ** 2).sum() #+ 0.0*lasso +0.01*ridge \n",
    "\n",
    "                        #print (gx.shape)\n",
    "\n",
    "                        grad_count += gx.nelement()\n",
    "\n",
    "\n",
    "                        #if count == 9:\n",
    "                        #    break\n",
    "                        #count=count+1\n",
    "                    # grad_diff = grad_diff / grad_count * 1000\n",
    "\n",
    "                    #grad_diff += ((original_pred[item]-pred)**2).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    grad_diff.backward()\n",
    "                    #print (count)\n",
    "\n",
    "                    #print (dummy_dy_dx)\n",
    "                    #print (original_dy_dx)\n",
    "\n",
    "\n",
    "                    return grad_diff\n",
    "\n",
    "\n",
    "\n",
    "                optimizer.step(closure)\n",
    "                if iters % 5 == 0: \n",
    "                    current_loss = closure()\n",
    "                    #if iters == 0: \n",
    "                    print (\"%.8f\" % current_loss.item())\n",
    "                    #print(iters, \"%.8f\" % current_loss.item())\n",
    "                history.append(tt(dummy_data[0].cpu()))\n",
    "\n",
    "\n",
    "\n",
    "            #plt.figure(figsize=(18, 12))\n",
    "            #for i in range(60):\n",
    "            #  plt.subplot(6, 10, i + 1)\n",
    "            #  plt.imshow(history[i * 5])\n",
    "            #  plt.title(\"iter=%d\" % (i * 5))\n",
    "            #  plt.axis('off')\n",
    "\n",
    "            plt.figure(figsize=(12, 1.5))\n",
    "            #iter_idx = [0,20,40,60,80,100,120,140,160,180]\n",
    "            plt.figure(figsize=(6.5, 1.2))\n",
    "            #iter_idx = [0,1000,2000,3000,4000,5000]\n",
    "            iter_idx = [0,5,10,20,50,100]\n",
    "\n",
    "\n",
    "            for i in range(6):\n",
    "              plt.subplot(1, 6, i + 1)\n",
    "              plt.imshow(history[iter_idx[i]])\n",
    "              plt.title(\"iter=%d\" % (iter_idx[i]))\n",
    "              plt.axis('off')\n",
    "\n",
    "            #np.savetxt('ssim_random2',percept_dis,fmt=\"%4f\")\n",
    "            #np.savetxt('mse_random2',recover_dis,fmt=\"%4f\")\n",
    "\n",
    "            #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "            #plt.savefig(\"./attack_image/index_%s_rand_%s_label_%s\"%(item,rd, label_pred.item()))\n",
    "            #plt.clf()\n",
    "\n",
    "        duration = time.process_time()-start\n",
    "        #print (\"Running time is %.4f.\" %(duration/10.0) )\n",
    "        print (duration/10.0 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percept_dis[299]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recover_dis[299]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(60):\n",
    "  plt.subplot(6, 10, i + 1)\n",
    "  plt.imshow(history[i * 5])\n",
    "  plt.title(\"iter=%d\" % (i * 5))\n",
    "  plt.axis('off')\n",
    "print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Level 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_msssim import ssim\n",
    "\n",
    "#dgc_rate_list = [20,30,40,50,70, 80,90]\n",
    "\n",
    "#for epoch in range(1):\n",
    "#for dgc_rate in dgc_rate_list:\n",
    "gau_rate_list = [100]\n",
    "for gau_rate in gau_rate_list:\n",
    "\n",
    "    \n",
    "    print('now starting',gau_rate)\n",
    "\n",
    "    for iter_,data in enumerate(trainloader,1):\n",
    "        \n",
    "        if iter_ != 1:\n",
    "            break\n",
    "   \n",
    "        ######### honest partipant #########\n",
    "        img_index = 30   #use img_index\n",
    "        dst_pil = tt(dst_tensor[img_index][0].cpu())   #use img_index\n",
    "\n",
    "        gt_data = transform(dst_pil).to(device)\n",
    "        gt_data = torch.unsqueeze(gt_data,0)\n",
    "\n",
    "        gt_label = dst_tensor[img_index][1].long().to(device) #use img_index\n",
    "        gt_label = gt_label.view(1, )\n",
    "        gt_onehot_label = label_to_onehot(gt_label, num_classes=3)\n",
    "\n",
    "        #plt.imshow(dst_pil)\n",
    "        #plt.axis('off')\n",
    "        plt.savefig(\"sample_data_COVID_Xray.png\")\n",
    "\n",
    "\n",
    "\n",
    "        batch =2  #\n",
    "        for bat in range(batch-1):\n",
    "            dst_pil = tt(dst_tensor[img_index+1+bat][0].cpu())   #use img_index\n",
    "            tmp = torch.unsqueeze(transform(dst_pil).to(device),0)\n",
    "            #print(tmp.shape)\n",
    "            gt_data = torch.cat((gt_data,tmp),0)\n",
    "\n",
    "            gt_label_tmp = dst_tensor[img_index+1+bat][1].long().to(device) #use img_index\n",
    "            gt_label_tmp = gt_label_tmp.view(1, )\n",
    "            gt_label = torch.cat((gt_label,gt_label_tmp),0)\n",
    "            gt_onehot_label = torch.cat((gt_onehot_label,label_to_onehot(gt_label_tmp, num_classes=3)),0)\n",
    "\n",
    "            plt.imshow(dst_pil)\n",
    "            #plt.savefig(\"./original/index_%s_label_%s\"%(bat+1,gt_label_tmp.item()))\n",
    "\n",
    "            #plt.title(\"Ground truth image\")\n",
    "            #print(\"GT label is %d.\" % gt_label.item(), \"\\nOnehot label is %d.\" % torch.argmax(gt_onehot_label, dim=-1).item())\n",
    "\n",
    "\n",
    "        gt_label = torch.reshape(gt_label,(-1,1))    \n",
    "        #print (gt_data.shape)\n",
    "        #print (gt_label.shape)\n",
    "        #print (gt_onehot_label.shape)\n",
    "\n",
    "\n",
    "        # compute original gradient \n",
    "        dy_dx = []\n",
    "        original_dy_dx=[]\n",
    "        original_pred = []\n",
    "        for item in range(batch):\n",
    "            gt_data_single = torch.unsqueeze(gt_data[item],0)\n",
    "            out = net(gt_data_single)\n",
    "            #y = criterion(out, gt_onehot_label[item])\n",
    "            y = criterion(out, gt_label[item])\n",
    "            dy_dx = torch.autograd.grad(y, net.parameters(),retain_graph=True)\n",
    "            original_dy_dx_tmp = list((_.detach().clone() for _ in dy_dx))\n",
    "            original_dy_dx.append(original_dy_dx_tmp)\n",
    "            out_tmp = out.detach().clone()\n",
    "            original_pred.append(out_tmp)\n",
    "            \n",
    "            \n",
    "        #if gaussian noise or laplace\n",
    "        m = torch.distributions.laplace.Laplace(torch.tensor([0.0]), torch.tensor([1/gau_rate]))\n",
    "        for item in range(batch):\n",
    "            for layer_idx in range(10):\n",
    "                #original_dy_dx[item][layer_idx] =  original_dy_dx[item][layer_idx] + torch.empty(original_dy_dx[item][layer_idx].size()).normal_(mean=0,std=1/gau_rate).to(device)\n",
    "                original_dy_dx[item][layer_idx] =  original_dy_dx[item][layer_idx] + torch.squeeze(m.sample(sample_shape=original_dy_dx[item][layer_idx].size()),dim=-1).to(device)\n",
    "                #break\n",
    "        ##if deep gradient compression\n",
    "        #print (original_dy_dx[0][0][0])\n",
    "        #for item in range(batch):\n",
    "        #    for layer_idx in range(10):\n",
    "        #        if layer_idx == 0:    \n",
    "        #            flat_dy_dx = torch.flatten(original_dy_dx[item][layer_idx])\n",
    "        #        else:\n",
    "        #            flat_dy_dx = torch.cat((flat_dy_dx,torch.flatten(original_dy_dx[item][layer_idx])),0)\n",
    "        #sorted_dy_dx = flat_dy_dx.abs().sort()\n",
    "        #size = np.asarray(list(flat_dy_dx.shape))\n",
    "        #thresh = sorted_dy_dx[0][int(size * dgc_rate/100.0)]\n",
    "        #print (size)\n",
    "        #print (int(size * dgc_rate/100.0))\n",
    "        #print (thresh)\n",
    "        #print (sorted_dy_dx[0][-1])\n",
    "        #for item in range(batch):\n",
    "        #    for layer_idx in range(10):\n",
    "        #        shape_tmp = original_dy_dx[item][layer_idx].size()\n",
    "        #        flat_dy_dx_prune = torch.flatten(original_dy_dx[item][layer_idx])\n",
    "        #        size_tmp = np.asarray(list(flat_dy_dx_prune.shape))\n",
    "        #        for m in range(int(size_tmp)):\n",
    "        #            if flat_dy_dx_prune[m].abs()<=thresh:\n",
    "        #                flat_dy_dx_prune[m] = 0\n",
    "        #        original_dy_dx[item][layer_idx] = flat_dy_dx_prune.view(shape_tmp)\n",
    "        #print (original_dy_dx[0][0][0])\n",
    "              \n",
    "\n",
    "        # generate dummy data and label\n",
    "        import time\n",
    "\n",
    "        #if iter_ % 10 ==0: \n",
    "        if iter_ == 1:\n",
    "            \n",
    "            #print ('epoch',epoch,'iter',iter_)\n",
    "            for item in range(1):\n",
    "                start = time.process_time()\n",
    "                for rd in range(1):\n",
    "\n",
    "                    torch.manual_seed(100*rd)\n",
    "\n",
    "                    pat_1 = torch.rand([3,16,16])\n",
    "                    pat_2 = torch.cat((pat_1,pat_1),dim=1)\n",
    "                    pat_4 = torch.cat((pat_2,pat_2),dim=2)\n",
    "                    dummy_data = torch.unsqueeze(pat_4,dim=0).to(device).requires_grad_(True)     \n",
    "\n",
    "                    dummy_unsqueeze=torch.unsqueeze(gt_onehot_label[item],dim=0)\n",
    "\n",
    "                    dummy_label = torch.randn(gt_onehot_label[item].size()).to(device).requires_grad_(True)\n",
    "                    label_pred = torch.argmin(torch.sum(original_dy_dx[item][-2], dim=-1), dim=-1).detach().reshape((1,)).requires_grad_(False)\n",
    "                    label_pred_onehot = label_to_onehot(label_pred, num_classes=3)\n",
    "\n",
    "                    plt.imshow(tt(dummy_data[0].cpu()))\n",
    "                    plt.title(\"Dummy data\")\n",
    "                    #plt.savefig(\"./random_seed/index_%s_rand_seed_%s_label_%s\"%(item,rd,torch.argmax(dummy_label, dim=-1).item()))\n",
    "\n",
    "                    plt.clf()\n",
    "                    #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "                    #print(\"stolen label is %d.\" % label_pred.item())\n",
    "\n",
    "                    #optimizer = torch.optim.LBFGS([dummy_data,dummy_label])\n",
    "                    optimizer = torch.optim.LBFGS([dummy_data,])\n",
    "\n",
    "\n",
    "                    history = []\n",
    "                    history_batch = []\n",
    "                    history_grad = []\n",
    "\n",
    "                    percept_dis = np.zeros(100)\n",
    "                    recover_dis = np.zeros(100)\n",
    "                    for iters in range(100):\n",
    "\n",
    "                        percept_dis[iters]=ssim(dummy_data,torch.unsqueeze(gt_data[item],dim=0),data_range=0).item()\n",
    "                        recover_dis[iters]=torch.dist(dummy_data,torch.unsqueeze(gt_data[item],dim=0),2).item()\n",
    "\n",
    "                        history.append(tt(dummy_data[0].cpu()))\n",
    "\n",
    "                        def closure():\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                            pred = net(dummy_data) \n",
    "                            dummy_onehot_label = F.softmax(dummy_label, dim=-1)\n",
    "                            #dummy_loss = criterion(pred, dummy_onehot_label) # TODO: fix the gt_label to dummy_label in both code and slides.\n",
    "\n",
    "                            #dummy_loss = criterion(pred, label_pred_onehot)\n",
    "                            dummy_loss = criterion(pred, label_pred)\n",
    "\n",
    "\n",
    "                            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "                            #dummy_dy_dp = torch.autograd.grad(dummy_loss, dummy_data, create_graph=True)\n",
    "                            #print (dummy_dy_dp[0].shape)\n",
    "\n",
    "                            grad_diff = 0\n",
    "                            grad_count = 0\n",
    "                            #count =0\n",
    "                            for gx, gy in zip(dummy_dy_dx, original_dy_dx[item]): # TODO: fix the variablas here\n",
    "\n",
    "                                #if iters==500 or iters== 1200:\n",
    "                                #    print (gx[0])\n",
    "                                #    print ('hahaha')\n",
    "                                #    print (gy[0])\n",
    "                                lasso = torch.norm(dummy_data,p=1)\n",
    "                                ridge = torch.norm(dummy_data,p=2)\n",
    "                                grad_diff += ((gx - gy) ** 2).sum() #+ 0.0*lasso +0.01*ridge\n",
    "\n",
    "\n",
    "                                grad_count += gx.nelement()\n",
    "\n",
    "                                #if count == 9:\n",
    "                                #    break\n",
    "                                #count=count+1\n",
    "                            # grad_diff = grad_diff / grad_count * 1000\n",
    "                            grad_diff.backward()\n",
    "                            #print (count)\n",
    "\n",
    "                            #print (dummy_dy_dx)\n",
    "                            #print (original_dy_dx)\n",
    "\n",
    "\n",
    "                            return grad_diff\n",
    "\n",
    "\n",
    "\n",
    "                        optimizer.step(closure)\n",
    "                        if iters % 5 == 0: \n",
    "                            current_loss = closure()\n",
    "                            #if iters == 0: \n",
    "                            #print (\"%.8f\" % current_loss.item())\n",
    "                            #print(iters, \"%.8f\" % current_loss.item())\n",
    "\n",
    "                    #     for bat in range(batch-1):\n",
    "                    #         history_batch.append(tt(dummy_data[bat].cpu()))\n",
    "\n",
    "                    #plt.figure(figsize=(30, 20))\n",
    "                    #for i in range(100):\n",
    "                    #    plt.subplot(10, 10, i + 1)\n",
    "                    #    plt.imshow(history[i * 5])\n",
    "                    #    plt.title(\"iter=%d\" % (i * 5))\n",
    "                    #    plt.axis('off')\n",
    "                    #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "\n",
    "                    #np.savetxt('./attack_image/lfw_ssim_idx_%s_laplace_%s'%(item,gau_rate),percept_dis,fmt=\"%4f\")\n",
    "                    #np.savetxt('./attack_image/lfw_mse_idx_%s_laplace_%s'%(item,gau_rate),recover_dis,fmt=\"%4f\")\n",
    "                    #plt.savefig(\"./attack_image/lfw_index_%s_laplace_%s\"%(item,gau_rate))\n",
    "\n",
    "                    #plt.clf()\n",
    "                    \n",
    "                    pinp = np.argmin(recover_dis)\n",
    "                    plt.imshow(history[pinp])\n",
    "                    plt.axis('off')\n",
    "                    \n",
    "                    #plt.figure(figsize=(15, 10))\n",
    "                    #for i in range(60):\n",
    "                    #    plt.subplot(6, 10, i + 1)\n",
    "                    #    plt.imshow(history[i * 14 ], cmap='gray')\n",
    "                    #    plt.title(\"iter=%d\" % (i*14))\n",
    "                    #    plt.axis('off')\n",
    "                    #plt.savefig(\"/content/sample_data\"%(item,gau_rate))\n",
    "                    \n",
    "                #duration = time.clock()-start\n",
    "                #print (\"Running time is %.4f.\" %(duration/10.0) )\n",
    "                #print (duration)\n",
    "                \n",
    "        \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "################################################### training when set to epoch\n",
    "\n",
    "#         #if epoch>=1:\n",
    "#         #if i==1:\n",
    "#             #break\n",
    "#         #print (iter_)\n",
    "#         inputs,label = data\n",
    "\n",
    "#         inputs,label =  Variable(inputs).to(device),Variable(label).to(device)\n",
    "\n",
    "#         optimizer_train.zero_grad()\n",
    "\n",
    "\n",
    "#         outputs_benign=net(inputs)\n",
    "#         #outputs_benign = F.softmax(outputs_benign, dim=-1)\n",
    "#         #print (outputs_benign[0])\n",
    "\n",
    "\n",
    "#         loss_benign =  criterion_train(outputs_benign,label)\n",
    "\n",
    "#         #print(\"loss computed\")\n",
    "#         loss_benign.backward()\n",
    "#         #print(\"loss BP\")\n",
    "#         optimizer_train.step()\n",
    "#         #sgd_update(net.parameters())\n",
    "\n",
    "#         #if i%2000==0:\n",
    "#         #print (loss_benign.item())\n",
    "#         #torch.save(net.state_dict(),'./LFW_net.pth')  \n",
    "\n",
    "#         #if  iter_%50==0:\n",
    "#         #    print ('attack',iter_)\n",
    "       \n",
    "        \n",
    "#         print ('fininshed training')\n",
    "#         break\n",
    "###############################   testing\n",
    "    \n",
    "#         total = len(y_test)\n",
    "#         acc =0.0\n",
    "#         for ct in range(total):\n",
    "#             testing_data = tt(testing[ct][0].cpu())\n",
    "#             testing_data1 = tp(testing_data).to(device)\n",
    "#             testing_data2 = testing_data1.view(1, *testing_data1.size())\n",
    "#             y_pred = net(testing_data2)\n",
    "#             predicted = torch.argmax(y_pred)\n",
    "\n",
    "#             if predicted == y_test[ct]:\n",
    "#                 acc=acc+1\n",
    "#         accuracy = acc / total\n",
    "#         print (accuracy)\n",
    "#         print ('fininshed testing')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_msssim import ssim\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    for iter_,data in enumerate(trainloader,1):\n",
    "   \n",
    "        ######### honest partipant #########\n",
    "        img_index = 30   #use img_index\n",
    "        dst_pil = tt(dst_tensor[img_index][0].cpu())   #use img_index\n",
    "\n",
    "        gt_data = transform(dst_pil).to(device)\n",
    "        gt_data = torch.unsqueeze(gt_data,0)\n",
    "\n",
    "        gt_label = dst_tensor[img_index][1].long().to(device) #use img_index\n",
    "        gt_label = gt_label.view(1, )\n",
    "        gt_onehot_label = label_to_onehot(gt_label, num_classes=3)\n",
    "\n",
    "        plt.imshow(dst_pil)\n",
    "        #plt.savefig(\"./original/index_%s_label_%s\"%(img_index,gt_label.item()))\n",
    "\n",
    "\n",
    "\n",
    "        batch =32  #\n",
    "        for bat in range(batch-1):\n",
    "            dst_pil = tt(dst_tensor[img_index+1+bat][0].cpu())   #use img_index\n",
    "            tmp = torch.unsqueeze(transform(dst_pil).to(device),0)\n",
    "            #print(tmp.shape)\n",
    "            gt_data = torch.cat((gt_data,tmp),0)\n",
    "\n",
    "            gt_label_tmp = dst_tensor[img_index+1+bat][1].long().to(device) #use img_index\n",
    "            gt_label_tmp = gt_label_tmp.view(1, )\n",
    "            gt_label = torch.cat((gt_label,gt_label_tmp),0)\n",
    "            gt_onehot_label = torch.cat((gt_onehot_label,label_to_onehot(gt_label_tmp, num_classes=3)),0)\n",
    "\n",
    "            plt.imshow(dst_pil)\n",
    "            #plt.savefig(\"./original/index_%s_label_%s\"%(bat+1,gt_label_tmp.item()))\n",
    "\n",
    "            #plt.title(\"Ground truth image\")\n",
    "            #print(\"GT label is %d.\" % gt_label.item(), \"\\nOnehot label is %d.\" % torch.argmax(gt_onehot_label, dim=-1).item())\n",
    "\n",
    "\n",
    "        gt_label = torch.reshape(gt_label,(-1,1))    \n",
    "        print (\"Image Shape = \",gt_data.shape)\n",
    "        print (\"Label shape = \",gt_label.shape)\n",
    "        print (\"Onehot Label shape = \",gt_onehot_label.shape)\n",
    "\n",
    "\n",
    "        # compute original gradient \n",
    "        dy_dx = []\n",
    "        original_dy_dx=[]\n",
    "        original_pred = []\n",
    "        for item in range(batch):\n",
    "            gt_data_single = torch.unsqueeze(gt_data[item],0)\n",
    "            out = net(gt_data_single)\n",
    "            #y = criterion(out, gt_onehot_label[item])\n",
    "            y = criterion(out, gt_label[item])\n",
    "            dy_dx = torch.autograd.grad(y, net.parameters(),retain_graph=True)\n",
    "            original_dy_dx_tmp = list((_.detach().clone() for _ in dy_dx))\n",
    "            original_dy_dx.append(original_dy_dx_tmp)\n",
    "            out_tmp = out.detach().clone()\n",
    "            original_pred.append(out_tmp)\n",
    "\n",
    "            #dy_dx.append(torch.autograd.grad(y, net.parameters()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # share the gradients with other clients\n",
    "        #original_dy_dx = list((_.detach().clone() for _ in dy_dx))\n",
    "\n",
    "\n",
    "        # generate dummy data and label\n",
    "        import time\n",
    "\n",
    "\n",
    "        for item in range(1):\n",
    "            start = time.process_time()\n",
    "            for rd in range(1):\n",
    "\n",
    "                torch.manual_seed(100*rd)\n",
    "        \n",
    "                pat_1 = torch.rand([3,16,16])\n",
    "                pat_2 = torch.cat((pat_1,pat_1),dim=1)\n",
    "                pat_4 = torch.cat((pat_2,pat_2),dim=2)\n",
    "                dummy_data = torch.unsqueeze(pat_4,dim=0).to(device).requires_grad_(True)     \n",
    "\n",
    "                dummy_unsqueeze=torch.unsqueeze(gt_onehot_label[item],dim=0)\n",
    "\n",
    "                dummy_label = torch.randn(dummy_unsqueeze.size()).to(device).requires_grad_(True)\n",
    "                label_pred=torch.argmin(torch.sum(original_dy_dx[item][-2], dim=-1), \n",
    "                                        dim=-1).detach().reshape((1,)).requires_grad_(False)\n",
    "                #print (original_dy_dx[item][-1].shape)\n",
    "                #print (original_dy_dx[item][-1].argmin())\n",
    "\n",
    "                #print (torch.sum(original_dy_dx[item][-2], dim=-1).argmin())\n",
    "\n",
    "                plt.imshow(tt(dummy_data[0].cpu()))\n",
    "                plt.title(\"Dummy data\")\n",
    "                #plt.savefig(\"./random_seed/index_%s_rand_seed_%s_label_%s\"%(item,rd,torch.argmax(dummy_label, dim=-1).item()))\n",
    "\n",
    "                plt.clf()\n",
    "                print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "                print(\"stolen label is %d.\" % label_pred.item())\n",
    "\n",
    "\n",
    "                #optimizer = torch.optim.LBFGS([dummy_data,dummy_label])\n",
    "                optimizer = torch.optim.LBFGS([dummy_data,])\n",
    "                #optimizer = torch.optim.AdamW([dummy_data,],lr=0.01)\n",
    "\n",
    "\n",
    "                history = []\n",
    "                history_batch = []\n",
    "                history_grad = []\n",
    "                \n",
    "                percept_dis = np.zeros(500)\n",
    "                recover_dis = np.zeros(500)\n",
    "                for iters in range(500):\n",
    "                    \n",
    "                    percept_dis[iters]=ssim(dummy_data,torch.unsqueeze(gt_data[item],dim=0),data_range=0).item()\n",
    "                    #recover_dis[iters]=torch.dist(dummy_data,torch.unsqueeze(gt_data[item],dim=0),2).item()\n",
    "                    recover_dis[iters]= F.mse_loss(dummy_data,torch.unsqueeze(gt_data[item],dim=0)).item()\n",
    "                    \n",
    "                    history.append(tt(dummy_data[0].cpu()))\n",
    "\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        pred = net(dummy_data) \n",
    "                        #dummy_onehot_label = F.softmax(dummy_label, dim=-1).long()\n",
    "\n",
    "                        #dummy_loss = criterion(pred, dummy_onehot_label) # TODO: fix the gt_label to dummy_label in both code and slides.\n",
    "                        #print (pred)\n",
    "                        #print (label_pred)\n",
    "\n",
    "                        dummy_loss = criterion(pred, label_pred)\n",
    "                        dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "                        #dummy_dy_dp = torch.autograd.grad(dummy_loss, dummy_data, create_graph=True)\n",
    "                        #print (dummy_dy_dp[0].shape)  \n",
    "\n",
    "                        grad_diff = 0\n",
    "                        grad_count = 0\n",
    "                        #count =0\n",
    "                        for gx, gy in zip(dummy_dy_dx, original_dy_dx[item]): # TODO: fix the variablas here\n",
    "\n",
    "                            #if iters==500 or iters== 1200:\n",
    "                            #print (gx[0])\n",
    "                            #    print ('hahaha')\n",
    "                            #print (gy[0])\n",
    "                            lasso = torch.norm(dummy_data,p=1)\n",
    "                            ridge = torch.norm(dummy_data,p=2)\n",
    "                            grad_diff += ((gx - gy) ** 2).sum() #+ 0.0*lasso +0.01*ridge \n",
    "\n",
    "                            #print (gx.shape)\n",
    "\n",
    "                            grad_count += gx.nelement()\n",
    "\n",
    "\n",
    "                            #if count == 9:\n",
    "                            #    break\n",
    "                            #count=count+1\n",
    "                        # grad_diff = grad_diff / grad_count * 1000\n",
    "\n",
    "                        #grad_diff += ((original_pred[item]-pred)**2).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        grad_diff.backward()\n",
    "                        #print (count)\n",
    "\n",
    "                        #print (dummy_dy_dx)\n",
    "                        #print (original_dy_dx)\n",
    "\n",
    "\n",
    "                        return grad_diff\n",
    "\n",
    "\n",
    "\n",
    "                    optimizer.step(closure)\n",
    "                    if iters % 5 == 0: \n",
    "                        current_loss = closure()\n",
    "                        #if iters == 0: \n",
    "                        #print (\"%.8f\" % current_loss.item())\n",
    "                        #print(iters, \"%.8f\" % current_loss.item())\n",
    "\n",
    "                #     for bat in range(batch-1):\n",
    "                #         history_batch.append(tt(dummy_data[bat].cpu()))\n",
    "\n",
    "                plt.figure(figsize=(30, 20))\n",
    "                for i in range(100):\n",
    "                    plt.subplot(10, 10, i + 1)\n",
    "                    plt.imshow(history[i * 5])\n",
    "                    plt.title(\"iter=%d\" % (i * 5))\n",
    "                    plt.axis('off')\n",
    "                #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "                \n",
    "                #np.savetxt('lfw_ssim_%s'%iter_,percept_dis,fmt=\"%4f\")\n",
    "                #np.savetxt('lfw_mse_%s'%iter_,recover_dis,fmt=\"%4f\")\n",
    "                #plt.savefig(\"./attack_image/index_%s_iter_%s_label_%s\"%(img_index,iter_,torch.argmax(dummy_label, dim=-1).item()))\n",
    "                \n",
    "                plt.clf()\n",
    "            duration = time.process_time()-start\n",
    "            #print (\"Running time is %.4f.\" %(duration/10.0) )\n",
    "            print (\"Duration = \", duration)\n",
    "            \n",
    "            \n",
    "            #if epoch>=1:\n",
    "        #if i==1:\n",
    "            #break\n",
    "        #print (iter_)\n",
    "        inputs,label = data\n",
    "\n",
    "        inputs,label =  Variable(inputs),Variable(label) \n",
    "\n",
    "        optimizer_train.zero_grad()\n",
    "\n",
    "\n",
    "        outputs_benign=net(inputs)\n",
    "        #outputs_benign = F.softmax(outputs_benign, dim=-1)\n",
    "        #print (outputs_benign[0])\n",
    "\n",
    "\n",
    "        loss_benign =  criterion_train(outputs_benign,label)\n",
    "\n",
    "        #print(\"loss computed\")\n",
    "        loss_benign.backward()\n",
    "        #print(\"loss BP\")\n",
    "        optimizer_train.step()\n",
    "\n",
    "        #if i%2000==0:\n",
    "        print (\"Loss Benign = \",loss_benign.item())\n",
    "        #torch.save(net.state_dict(),'./LFW_net.pth')  \n",
    "\n",
    "  \n",
    "        print ('fininshed training')\n",
    "        total = len(y_test)\n",
    "        acc =0.0\n",
    "        for ct in range(total):\n",
    "            testing_data = tt(testing[ct][0].cpu())\n",
    "            testing_data1 = transform(testing_data).to(device)\n",
    "            testing_data2 = testing_data1.view(1, *testing_data1.size())\n",
    "            y_pred = net(testing_data2)\n",
    "            predicted = torch.argmax(y_pred)\n",
    "\n",
    "            if predicted == y_test[ct]:\n",
    "                acc=acc+1\n",
    "        accuracy = acc / total\n",
    "        print (\"accuracy = \",accuracy)\n",
    "        print ('fininshed testing')\n",
    "        print(\"ssim_random2 = \", percept_dis)\n",
    "        print(\"mse_random2 = \", recover_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(60):\n",
    "  plt.subplot(6, 10, i + 1)\n",
    "  plt.imshow(history[i * 5])\n",
    "  plt.title(\"iter=%d\" % (i * 5))\n",
    "  plt.axis('off')\n",
    "print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(history[295])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[21], alpha =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Level 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 34606,
     "status": "ok",
     "timestamp": 1680756712333,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "HeFhMuZt20Gu",
    "outputId": "e14db101-033f-4627-c731-51acf951ca5d"
   },
   "outputs": [],
   "source": [
    "# from pytorch_msssim import ssim\n",
    "\n",
    "#dgc_rate_list = [20,30,40,50,70, 80,90]\n",
    "\n",
    "#for epoch in range(1):\n",
    "#for dgc_rate in dgc_rate_list:\n",
    "gau_rate_list = [200]\n",
    "for gau_rate in gau_rate_list:\n",
    "\n",
    "    \n",
    "    print('now starting',gau_rate)\n",
    "\n",
    "    for iter_,data in enumerate(trainloader,1):\n",
    "        \n",
    "        if iter_ != 1:\n",
    "            break\n",
    "   \n",
    "        ######### honest partipant #########\n",
    "        img_index = 30   #use img_index\n",
    "        dst_pil = tt(dst_tensor[img_index][0].cpu())   #use img_index\n",
    "\n",
    "        gt_data = transform(dst_pil).to(device)\n",
    "        gt_data = torch.unsqueeze(gt_data,0)\n",
    "\n",
    "        gt_label = dst_tensor[img_index][1].long().to(device) #use img_index\n",
    "        gt_label = gt_label.view(1, )\n",
    "        gt_onehot_label = label_to_onehot(gt_label, num_classes=3)\n",
    "\n",
    "        #plt.imshow(dst_pil)\n",
    "        #plt.axis('off')\n",
    "        plt.savefig(\"sample_data_COVID_Xray.png\")\n",
    "\n",
    "\n",
    "\n",
    "        batch =2  #\n",
    "        for bat in range(batch-1):\n",
    "            dst_pil = tt(dst_tensor[img_index+1+bat][0].cpu())   #use img_index\n",
    "            tmp = torch.unsqueeze(transform(dst_pil).to(device),0)\n",
    "            #print(tmp.shape)\n",
    "            gt_data = torch.cat((gt_data,tmp),0)\n",
    "\n",
    "            gt_label_tmp = dst_tensor[img_index+1+bat][1].long().to(device) #use img_index\n",
    "            gt_label_tmp = gt_label_tmp.view(1, )\n",
    "            gt_label = torch.cat((gt_label,gt_label_tmp),0)\n",
    "            gt_onehot_label = torch.cat((gt_onehot_label,label_to_onehot(gt_label_tmp, num_classes=3)),0)\n",
    "\n",
    "            plt.imshow(dst_pil)\n",
    "            #plt.savefig(\"./original/index_%s_label_%s\"%(bat+1,gt_label_tmp.item()))\n",
    "\n",
    "            #plt.title(\"Ground truth image\")\n",
    "            #print(\"GT label is %d.\" % gt_label.item(), \"\\nOnehot label is %d.\" % torch.argmax(gt_onehot_label, dim=-1).item())\n",
    "\n",
    "\n",
    "        gt_label = torch.reshape(gt_label,(-1,1))    \n",
    "        #print (gt_data.shape)\n",
    "        #print (gt_label.shape)\n",
    "        #print (gt_onehot_label.shape)\n",
    "\n",
    "\n",
    "        # compute original gradient \n",
    "        dy_dx = []\n",
    "        original_dy_dx=[]\n",
    "        original_pred = []\n",
    "        for item in range(batch):\n",
    "            gt_data_single = torch.unsqueeze(gt_data[item],0)\n",
    "            out = net(gt_data_single)\n",
    "            #y = criterion(out, gt_onehot_label[item])\n",
    "            y = criterion(out, gt_label[item])\n",
    "            dy_dx = torch.autograd.grad(y, net.parameters(),retain_graph=True)\n",
    "            original_dy_dx_tmp = list((_.detach().clone() for _ in dy_dx))\n",
    "            original_dy_dx.append(original_dy_dx_tmp)\n",
    "            out_tmp = out.detach().clone()\n",
    "            original_pred.append(out_tmp)\n",
    "            \n",
    "            \n",
    "        #if gaussian noise or laplace\n",
    "        m = torch.distributions.laplace.Laplace(torch.tensor([0.0]), torch.tensor([1/gau_rate]))\n",
    "        for item in range(batch):\n",
    "            for layer_idx in range(10):\n",
    "                #original_dy_dx[item][layer_idx] =  original_dy_dx[item][layer_idx] + torch.empty(original_dy_dx[item][layer_idx].size()).normal_(mean=0,std=1/gau_rate).to(device)\n",
    "                original_dy_dx[item][layer_idx] =  original_dy_dx[item][layer_idx] + torch.squeeze(m.sample(sample_shape=original_dy_dx[item][layer_idx].size()),dim=-1).to(device)\n",
    "                #break\n",
    "        ##if deep gradient compression\n",
    "        #print (original_dy_dx[0][0][0])\n",
    "        #for item in range(batch):\n",
    "        #    for layer_idx in range(10):\n",
    "        #        if layer_idx == 0:    \n",
    "        #            flat_dy_dx = torch.flatten(original_dy_dx[item][layer_idx])\n",
    "        #        else:\n",
    "        #            flat_dy_dx = torch.cat((flat_dy_dx,torch.flatten(original_dy_dx[item][layer_idx])),0)\n",
    "        #sorted_dy_dx = flat_dy_dx.abs().sort()\n",
    "        #size = np.asarray(list(flat_dy_dx.shape))\n",
    "        #thresh = sorted_dy_dx[0][int(size * dgc_rate/100.0)]\n",
    "        #print (size)\n",
    "        #print (int(size * dgc_rate/100.0))\n",
    "        #print (thresh)\n",
    "        #print (sorted_dy_dx[0][-1])\n",
    "        #for item in range(batch):\n",
    "        #    for layer_idx in range(10):\n",
    "        #        shape_tmp = original_dy_dx[item][layer_idx].size()\n",
    "        #        flat_dy_dx_prune = torch.flatten(original_dy_dx[item][layer_idx])\n",
    "        #        size_tmp = np.asarray(list(flat_dy_dx_prune.shape))\n",
    "        #        for m in range(int(size_tmp)):\n",
    "        #            if flat_dy_dx_prune[m].abs()<=thresh:\n",
    "        #                flat_dy_dx_prune[m] = 0\n",
    "        #        original_dy_dx[item][layer_idx] = flat_dy_dx_prune.view(shape_tmp)\n",
    "        #print (original_dy_dx[0][0][0])\n",
    "              \n",
    "\n",
    "        # generate dummy data and label\n",
    "        import time\n",
    "\n",
    "        #if iter_ % 10 ==0: \n",
    "        if iter_ == 1:\n",
    "            \n",
    "            #print ('epoch',epoch,'iter',iter_)\n",
    "            for item in range(1):\n",
    "                start = time.process_time()\n",
    "                for rd in range(1):\n",
    "\n",
    "                    torch.manual_seed(100*rd)\n",
    "\n",
    "                    pat_1 = torch.rand([3,16,16])\n",
    "                    pat_2 = torch.cat((pat_1,pat_1),dim=1)\n",
    "                    pat_4 = torch.cat((pat_2,pat_2),dim=2)\n",
    "                    dummy_data = torch.unsqueeze(pat_4,dim=0).to(device).requires_grad_(True)     \n",
    "\n",
    "                    dummy_unsqueeze=torch.unsqueeze(gt_onehot_label[item],dim=0)\n",
    "\n",
    "                    dummy_label = torch.randn(gt_onehot_label[item].size()).to(device).requires_grad_(True)\n",
    "                    label_pred = torch.argmin(torch.sum(original_dy_dx[item][-2], dim=-1), dim=-1).detach().reshape((1,)).requires_grad_(False)\n",
    "                    label_pred_onehot = label_to_onehot(label_pred, num_classes=3)\n",
    "\n",
    "                    plt.imshow(tt(dummy_data[0].cpu()))\n",
    "                    plt.title(\"Dummy data\")\n",
    "                    #plt.savefig(\"./random_seed/index_%s_rand_seed_%s_label_%s\"%(item,rd,torch.argmax(dummy_label, dim=-1).item()))\n",
    "\n",
    "                    plt.clf()\n",
    "                    #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "                    #print(\"stolen label is %d.\" % label_pred.item())\n",
    "\n",
    "                    #optimizer = torch.optim.LBFGS([dummy_data,dummy_label])\n",
    "                    optimizer = torch.optim.LBFGS([dummy_data,])\n",
    "\n",
    "\n",
    "                    history = []\n",
    "                    history_batch = []\n",
    "                    history_grad = []\n",
    "\n",
    "                    percept_dis = np.zeros(100)\n",
    "                    recover_dis = np.zeros(100)\n",
    "                    for iters in range(100):\n",
    "\n",
    "                        percept_dis[iters]=ssim(dummy_data,torch.unsqueeze(gt_data[item],dim=0),data_range=0).item()\n",
    "                        recover_dis[iters]=torch.dist(dummy_data,torch.unsqueeze(gt_data[item],dim=0),2).item()\n",
    "\n",
    "                        history.append(tt(dummy_data[0].cpu()))\n",
    "\n",
    "                        def closure():\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                            pred = net(dummy_data) \n",
    "                            dummy_onehot_label = F.softmax(dummy_label, dim=-1)\n",
    "                            #dummy_loss = criterion(pred, dummy_onehot_label) # TODO: fix the gt_label to dummy_label in both code and slides.\n",
    "\n",
    "                            #dummy_loss = criterion(pred, label_pred_onehot)\n",
    "                            dummy_loss = criterion(pred, label_pred)\n",
    "\n",
    "\n",
    "                            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "                            #dummy_dy_dp = torch.autograd.grad(dummy_loss, dummy_data, create_graph=True)\n",
    "                            #print (dummy_dy_dp[0].shape)\n",
    "\n",
    "                            grad_diff = 0\n",
    "                            grad_count = 0\n",
    "                            #count =0\n",
    "                            for gx, gy in zip(dummy_dy_dx, original_dy_dx[item]): # TODO: fix the variablas here\n",
    "\n",
    "                                #if iters==500 or iters== 1200:\n",
    "                                #    print (gx[0])\n",
    "                                #    print ('hahaha')\n",
    "                                #    print (gy[0])\n",
    "                                lasso = torch.norm(dummy_data,p=1)\n",
    "                                ridge = torch.norm(dummy_data,p=2)\n",
    "                                grad_diff += ((gx - gy) ** 2).sum() #+ 0.0*lasso +0.01*ridge\n",
    "\n",
    "\n",
    "                                grad_count += gx.nelement()\n",
    "\n",
    "                                #if count == 9:\n",
    "                                #    break\n",
    "                                #count=count+1\n",
    "                            # grad_diff = grad_diff / grad_count * 1000\n",
    "                            grad_diff.backward()\n",
    "                            #print (count)\n",
    "\n",
    "                            #print (dummy_dy_dx)\n",
    "                            #print (original_dy_dx)\n",
    "\n",
    "\n",
    "                            return grad_diff\n",
    "\n",
    "\n",
    "\n",
    "                        optimizer.step(closure)\n",
    "                        if iters % 5 == 0: \n",
    "                            current_loss = closure()\n",
    "                            #if iters == 0: \n",
    "                            #print (\"%.8f\" % current_loss.item())\n",
    "                            #print(iters, \"%.8f\" % current_loss.item())\n",
    "\n",
    "                    #     for bat in range(batch-1):\n",
    "                    #         history_batch.append(tt(dummy_data[bat].cpu()))\n",
    "\n",
    "                    #plt.figure(figsize=(30, 20))\n",
    "                    #for i in range(100):\n",
    "                    #    plt.subplot(10, 10, i + 1)\n",
    "                    #    plt.imshow(history[i * 5])\n",
    "                    #    plt.title(\"iter=%d\" % (i * 5))\n",
    "                    #    plt.axis('off')\n",
    "                    #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "\n",
    "                    #np.savetxt('./attack_image/lfw_ssim_idx_%s_laplace_%s'%(item,gau_rate),percept_dis,fmt=\"%4f\")\n",
    "                    #np.savetxt('./attack_image/lfw_mse_idx_%s_laplace_%s'%(item,gau_rate),recover_dis,fmt=\"%4f\")\n",
    "                    #plt.savefig(\"./attack_image/lfw_index_%s_laplace_%s\"%(item,gau_rate))\n",
    "\n",
    "                    #plt.clf()\n",
    "                    \n",
    "                    pinp = np.argmin(recover_dis)\n",
    "                    plt.imshow(history[pinp])\n",
    "                    plt.axis('off')\n",
    "                    \n",
    "                    #plt.figure(figsize=(15, 10))\n",
    "                    #for i in range(60):\n",
    "                    #    plt.subplot(6, 10, i + 1)\n",
    "                    #    plt.imshow(history[i * 14 ], cmap='gray')\n",
    "                    #    plt.title(\"iter=%d\" % (i*14))\n",
    "                    #    plt.axis('off')\n",
    "                    #plt.savefig(\"/content/sample_data\"%(item,gau_rate))\n",
    "                    \n",
    "                #duration = time.clock()-start\n",
    "                #print (\"Running time is %.4f.\" %(duration/10.0) )\n",
    "                #print (duration)\n",
    "                \n",
    "        \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "################################################### training when set to epoch\n",
    "\n",
    "#         #if epoch>=1:\n",
    "#         #if i==1:\n",
    "#             #break\n",
    "#         #print (iter_)\n",
    "#         inputs,label = data\n",
    "\n",
    "#         inputs,label =  Variable(inputs).to(device),Variable(label).to(device)\n",
    "\n",
    "#         optimizer_train.zero_grad()\n",
    "\n",
    "\n",
    "#         outputs_benign=net(inputs)\n",
    "#         #outputs_benign = F.softmax(outputs_benign, dim=-1)\n",
    "#         #print (outputs_benign[0])\n",
    "\n",
    "\n",
    "#         loss_benign =  criterion_train(outputs_benign,label)\n",
    "\n",
    "#         #print(\"loss computed\")\n",
    "#         loss_benign.backward()\n",
    "#         #print(\"loss BP\")\n",
    "#         optimizer_train.step()\n",
    "#         #sgd_update(net.parameters())\n",
    "\n",
    "#         #if i%2000==0:\n",
    "#         #print (loss_benign.item())\n",
    "#         #torch.save(net.state_dict(),'./LFW_net.pth')  \n",
    "\n",
    "#         #if  iter_%50==0:\n",
    "#         #    print ('attack',iter_)\n",
    "       \n",
    "        \n",
    "#         print ('fininshed training')\n",
    "#         break\n",
    "###############################   testing\n",
    "    \n",
    "#         total = len(y_test)\n",
    "#         acc =0.0\n",
    "#         for ct in range(total):\n",
    "#             testing_data = tt(testing[ct][0].cpu())\n",
    "#             testing_data1 = tp(testing_data).to(device)\n",
    "#             testing_data2 = testing_data1.view(1, *testing_data1.size())\n",
    "#             y_pred = net(testing_data2)\n",
    "#             predicted = torch.argmax(y_pred)\n",
    "\n",
    "#             if predicted == y_test[ct]:\n",
    "#                 acc=acc+1\n",
    "#         accuracy = acc / total\n",
    "#         print (accuracy)\n",
    "#         print ('fininshed testing')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 280467,
     "status": "ok",
     "timestamp": 1680756992797,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "1MXV1h5V20Gw",
    "outputId": "e86b9718-760c-496f-aecb-18de6fd137f9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pytorch_msssim import ssim\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    for iter_,data in enumerate(trainloader,1):\n",
    "   \n",
    "        ######### honest partipant #########\n",
    "        img_index = 30   #use img_index\n",
    "        dst_pil = tt(dst_tensor[img_index][0].cpu())   #use img_index\n",
    "\n",
    "        gt_data = transform(dst_pil).to(device)\n",
    "        gt_data = torch.unsqueeze(gt_data,0)\n",
    "\n",
    "        gt_label = dst_tensor[img_index][1].long().to(device) #use img_index\n",
    "        gt_label = gt_label.view(1, )\n",
    "        gt_onehot_label = label_to_onehot(gt_label, num_classes=3)\n",
    "\n",
    "        plt.imshow(dst_pil)\n",
    "        #plt.savefig(\"./original/index_%s_label_%s\"%(img_index,gt_label.item()))\n",
    "\n",
    "\n",
    "\n",
    "        batch =32  #\n",
    "        for bat in range(batch-1):\n",
    "            dst_pil = tt(dst_tensor[img_index+1+bat][0].cpu())   #use img_index\n",
    "            tmp = torch.unsqueeze(transform(dst_pil).to(device),0)\n",
    "            #print(tmp.shape)\n",
    "            gt_data = torch.cat((gt_data,tmp),0)\n",
    "\n",
    "            gt_label_tmp = dst_tensor[img_index+1+bat][1].long().to(device) #use img_index\n",
    "            gt_label_tmp = gt_label_tmp.view(1, )\n",
    "            gt_label = torch.cat((gt_label,gt_label_tmp),0)\n",
    "            gt_onehot_label = torch.cat((gt_onehot_label,label_to_onehot(gt_label_tmp, num_classes=3)),0)\n",
    "\n",
    "            plt.imshow(dst_pil)\n",
    "            #plt.savefig(\"./original/index_%s_label_%s\"%(bat+1,gt_label_tmp.item()))\n",
    "\n",
    "            #plt.title(\"Ground truth image\")\n",
    "            #print(\"GT label is %d.\" % gt_label.item(), \"\\nOnehot label is %d.\" % torch.argmax(gt_onehot_label, dim=-1).item())\n",
    "\n",
    "\n",
    "        gt_label = torch.reshape(gt_label,(-1,1))    \n",
    "        print (\"Image Shape = \",gt_data.shape)\n",
    "        print (\"Label shape = \",gt_label.shape)\n",
    "        print (\"Onehot Label shape = \",gt_onehot_label.shape)\n",
    "\n",
    "\n",
    "        # compute original gradient \n",
    "        dy_dx = []\n",
    "        original_dy_dx=[]\n",
    "        original_pred = []\n",
    "        for item in range(batch):\n",
    "            gt_data_single = torch.unsqueeze(gt_data[item],0)\n",
    "            out = net(gt_data_single)\n",
    "            #y = criterion(out, gt_onehot_label[item])\n",
    "            y = criterion(out, gt_label[item])\n",
    "            dy_dx = torch.autograd.grad(y, net.parameters(),retain_graph=True)\n",
    "            original_dy_dx_tmp = list((_.detach().clone() for _ in dy_dx))\n",
    "            original_dy_dx.append(original_dy_dx_tmp)\n",
    "            out_tmp = out.detach().clone()\n",
    "            original_pred.append(out_tmp)\n",
    "\n",
    "            #dy_dx.append(torch.autograd.grad(y, net.parameters()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # share the gradients with other clients\n",
    "        #original_dy_dx = list((_.detach().clone() for _ in dy_dx))\n",
    "\n",
    "\n",
    "        # generate dummy data and label\n",
    "        import time\n",
    "\n",
    "\n",
    "        for item in range(1):\n",
    "            start = time.process_time()\n",
    "            for rd in range(1):\n",
    "\n",
    "                torch.manual_seed(100*rd)\n",
    "        \n",
    "                pat_1 = torch.rand([3,16,16])\n",
    "                pat_2 = torch.cat((pat_1,pat_1),dim=1)\n",
    "                pat_4 = torch.cat((pat_2,pat_2),dim=2)\n",
    "                dummy_data = torch.unsqueeze(pat_4,dim=0).to(device).requires_grad_(True)     \n",
    "\n",
    "                dummy_unsqueeze=torch.unsqueeze(gt_onehot_label[item],dim=0)\n",
    "\n",
    "                dummy_label = torch.randn(dummy_unsqueeze.size()).to(device).requires_grad_(True)\n",
    "                label_pred=torch.argmin(torch.sum(original_dy_dx[item][-2], dim=-1), \n",
    "                                        dim=-1).detach().reshape((1,)).requires_grad_(False)\n",
    "                #print (original_dy_dx[item][-1].shape)\n",
    "                #print (original_dy_dx[item][-1].argmin())\n",
    "\n",
    "                #print (torch.sum(original_dy_dx[item][-2], dim=-1).argmin())\n",
    "\n",
    "                plt.imshow(tt(dummy_data[0].cpu()))\n",
    "                plt.title(\"Dummy data\")\n",
    "                #plt.savefig(\"./random_seed/index_%s_rand_seed_%s_label_%s\"%(item,rd,torch.argmax(dummy_label, dim=-1).item()))\n",
    "\n",
    "                plt.clf()\n",
    "                print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "                print(\"stolen label is %d.\" % label_pred.item())\n",
    "\n",
    "\n",
    "                #optimizer = torch.optim.LBFGS([dummy_data,dummy_label])\n",
    "                optimizer = torch.optim.LBFGS([dummy_data,])\n",
    "                #optimizer = torch.optim.AdamW([dummy_data,],lr=0.01)\n",
    "\n",
    "\n",
    "                history = []\n",
    "                history_batch = []\n",
    "                history_grad = []\n",
    "                \n",
    "                percept_dis = np.zeros(500)\n",
    "                recover_dis = np.zeros(500)\n",
    "                for iters in range(500):\n",
    "                    \n",
    "                    percept_dis[iters]=ssim(dummy_data,torch.unsqueeze(gt_data[item],dim=0),data_range=0).item()\n",
    "                    #recover_dis[iters]=torch.dist(dummy_data,torch.unsqueeze(gt_data[item],dim=0),2).item()\n",
    "                    recover_dis[iters]= F.mse_loss(dummy_data,torch.unsqueeze(gt_data[item],dim=0)).item()\n",
    "                    \n",
    "                    history.append(tt(dummy_data[0].cpu()))\n",
    "\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        pred = net(dummy_data) \n",
    "                        #dummy_onehot_label = F.softmax(dummy_label, dim=-1).long()\n",
    "\n",
    "                        #dummy_loss = criterion(pred, dummy_onehot_label) # TODO: fix the gt_label to dummy_label in both code and slides.\n",
    "                        #print (pred)\n",
    "                        #print (label_pred)\n",
    "\n",
    "                        dummy_loss = criterion(pred, label_pred)\n",
    "                        dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "                        #dummy_dy_dp = torch.autograd.grad(dummy_loss, dummy_data, create_graph=True)\n",
    "                        #print (dummy_dy_dp[0].shape)  \n",
    "\n",
    "                        grad_diff = 0\n",
    "                        grad_count = 0\n",
    "                        #count =0\n",
    "                        for gx, gy in zip(dummy_dy_dx, original_dy_dx[item]): # TODO: fix the variablas here\n",
    "\n",
    "                            #if iters==500 or iters== 1200:\n",
    "                            #print (gx[0])\n",
    "                            #    print ('hahaha')\n",
    "                            #print (gy[0])\n",
    "                            lasso = torch.norm(dummy_data,p=1)\n",
    "                            ridge = torch.norm(dummy_data,p=2)\n",
    "                            grad_diff += ((gx - gy) ** 2).sum() #+ 0.0*lasso +0.01*ridge \n",
    "\n",
    "                            #print (gx.shape)\n",
    "\n",
    "                            grad_count += gx.nelement()\n",
    "\n",
    "\n",
    "                            #if count == 9:\n",
    "                            #    break\n",
    "                            #count=count+1\n",
    "                        # grad_diff = grad_diff / grad_count * 1000\n",
    "\n",
    "                        #grad_diff += ((original_pred[item]-pred)**2).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        grad_diff.backward()\n",
    "                        #print (count)\n",
    "\n",
    "                        #print (dummy_dy_dx)\n",
    "                        #print (original_dy_dx)\n",
    "\n",
    "\n",
    "                        return grad_diff\n",
    "\n",
    "\n",
    "\n",
    "                    optimizer.step(closure)\n",
    "                    if iters % 5 == 0: \n",
    "                        current_loss = closure()\n",
    "                        #if iters == 0: \n",
    "                        #print (\"%.8f\" % current_loss.item())\n",
    "                        #print(iters, \"%.8f\" % current_loss.item())\n",
    "\n",
    "                #     for bat in range(batch-1):\n",
    "                #         history_batch.append(tt(dummy_data[bat].cpu()))\n",
    "\n",
    "                plt.figure(figsize=(30, 20))\n",
    "                for i in range(100):\n",
    "                    plt.subplot(10, 10, i + 1)\n",
    "                    plt.imshow(history[i * 5])\n",
    "                    plt.title(\"iter=%d\" % (i * 5))\n",
    "                    plt.axis('off')\n",
    "                #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "                \n",
    "                #np.savetxt('lfw_ssim_%s'%iter_,percept_dis,fmt=\"%4f\")\n",
    "                #np.savetxt('lfw_mse_%s'%iter_,recover_dis,fmt=\"%4f\")\n",
    "                #plt.savefig(\"./attack_image/index_%s_iter_%s_label_%s\"%(img_index,iter_,torch.argmax(dummy_label, dim=-1).item()))\n",
    "                \n",
    "                plt.clf()\n",
    "            duration = time.process_time()-start\n",
    "            #print (\"Running time is %.4f.\" %(duration/10.0) )\n",
    "            print (\"Duration = \", duration)\n",
    "            \n",
    "            \n",
    "            #if epoch>=1:\n",
    "        #if i==1:\n",
    "            #break\n",
    "        #print (iter_)\n",
    "        inputs,label = data\n",
    "\n",
    "        inputs,label =  Variable(inputs),Variable(label) \n",
    "\n",
    "        optimizer_train.zero_grad()\n",
    "\n",
    "\n",
    "        outputs_benign=net(inputs)\n",
    "        #outputs_benign = F.softmax(outputs_benign, dim=-1)\n",
    "        #print (outputs_benign[0])\n",
    "\n",
    "\n",
    "        loss_benign =  criterion_train(outputs_benign,label)\n",
    "\n",
    "        #print(\"loss computed\")\n",
    "        loss_benign.backward()\n",
    "        #print(\"loss BP\")\n",
    "        optimizer_train.step()\n",
    "\n",
    "        #if i%2000==0:\n",
    "        print (\"Loss Benign = \",loss_benign.item())\n",
    "        #torch.save(net.state_dict(),'./LFW_net.pth')  \n",
    "\n",
    "  \n",
    "        print ('fininshed training')\n",
    "        total = len(y_test)\n",
    "        acc =0.0\n",
    "        for ct in range(total):\n",
    "            testing_data = tt(testing[ct][0].cpu())\n",
    "            testing_data1 = transform(testing_data).to(device)\n",
    "            testing_data2 = testing_data1.view(1, *testing_data1.size())\n",
    "            y_pred = net(testing_data2)\n",
    "            predicted = torch.argmax(y_pred)\n",
    "\n",
    "            if predicted == y_test[ct]:\n",
    "                acc=acc+1\n",
    "        accuracy = acc / total\n",
    "        print (\"accuracy = \",accuracy)\n",
    "        print ('fininshed testing')\n",
    "        print(\"ssim_random2 = \", percept_dis)\n",
    "        print(\"mse_random2 = \", recover_dis)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1680756992797,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "aCN-c2S6i0xT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "executionInfo": {
     "elapsed": 6027,
     "status": "ok",
     "timestamp": 1680756998821,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "aokP-jhal96-",
    "outputId": "98a63eb0-06bf-45e9-a0ad-a99dc0af44d1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(60):\n",
    "  plt.subplot(6, 10, i + 1)\n",
    "  plt.imshow(history[i * 5])\n",
    "  plt.title(\"iter=%d\" % (i * 5))\n",
    "  plt.axis('off')\n",
    "print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1680756998821,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "pmarD6w620Gy"
   },
   "source": [
    "# Noise Level 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1680756998822,
     "user": {
      "displayName": "Badhan Chandra Das",
      "userId": "08979688857117615126"
     },
     "user_tz": 240
    },
    "id": "g82gXT3flRAh"
   },
   "outputs": [],
   "source": [
    "from pytorch_msssim import ssim\n",
    "\n",
    "#dgc_rate_list = [20,30,40,50,70, 80,90]\n",
    "\n",
    "#for epoch in range(1):\n",
    "#for dgc_rate in dgc_rate_list:\n",
    "gau_rate_list = [300]\n",
    "for gau_rate in gau_rate_list:\n",
    "\n",
    "    \n",
    "    print('now starting',gau_rate)\n",
    "\n",
    "    for iter_,data in enumerate(trainloader,1):\n",
    "        \n",
    "        if iter_ != 1:\n",
    "            break\n",
    "   \n",
    "        ######### honest partipant #########\n",
    "        img_index = 30   #use img_index\n",
    "        dst_pil = tt(dst_tensor[img_index][0].cpu())   #use img_index\n",
    "\n",
    "        gt_data = transform(dst_pil).to(device)\n",
    "        gt_data = torch.unsqueeze(gt_data,0)\n",
    "\n",
    "        gt_label = dst_tensor[img_index][1].long().to(device) #use img_index\n",
    "        gt_label = gt_label.view(1, )\n",
    "        gt_onehot_label = label_to_onehot(gt_label, num_classes=3)\n",
    "\n",
    "        #plt.imshow(dst_pil)\n",
    "        #plt.axis('off')\n",
    "        #plt.savefig(\"/content/sample_data\")\n",
    "\n",
    "\n",
    "\n",
    "        batch =2  #\n",
    "        for bat in range(batch-1):\n",
    "            dst_pil = tt(dst_tensor[img_index+1+bat][0].cpu())   #use img_index\n",
    "            tmp = torch.unsqueeze(transform(dst_pil).to(device),0)\n",
    "            #print(tmp.shape)\n",
    "            gt_data = torch.cat((gt_data,tmp),0)\n",
    "\n",
    "            gt_label_tmp = dst_tensor[img_index+1+bat][1].long().to(device) #use img_index\n",
    "            gt_label_tmp = gt_label_tmp.view(1, )\n",
    "            gt_label = torch.cat((gt_label,gt_label_tmp),0)\n",
    "            gt_onehot_label = torch.cat((gt_onehot_label,label_to_onehot(gt_label_tmp, num_classes=3)),0)\n",
    "\n",
    "            plt.imshow(dst_pil)\n",
    "            #plt.savefig(\"./original/index_%s_label_%s\"%(bat+1,gt_label_tmp.item()))\n",
    "\n",
    "            #plt.title(\"Ground truth image\")\n",
    "            #print(\"GT label is %d.\" % gt_label.item(), \"\\nOnehot label is %d.\" % torch.argmax(gt_onehot_label, dim=-1).item())\n",
    "\n",
    "\n",
    "        gt_label = torch.reshape(gt_label,(-1,1))    \n",
    "        #print (gt_data.shape)\n",
    "        #print (gt_label.shape)\n",
    "        #print (gt_onehot_label.shape)\n",
    "\n",
    "\n",
    "        # compute original gradient \n",
    "        dy_dx = []\n",
    "        original_dy_dx=[]\n",
    "        original_pred = []\n",
    "        for item in range(batch):\n",
    "            gt_data_single = torch.unsqueeze(gt_data[item],0)\n",
    "            out = net(gt_data_single)\n",
    "            #y = criterion(out, gt_onehot_label[item])\n",
    "            y = criterion(out, gt_label[item])\n",
    "            dy_dx = torch.autograd.grad(y, net.parameters(),retain_graph=True)\n",
    "            original_dy_dx_tmp = list((_.detach().clone() for _ in dy_dx))\n",
    "            original_dy_dx.append(original_dy_dx_tmp)\n",
    "            out_tmp = out.detach().clone()\n",
    "            original_pred.append(out_tmp)\n",
    "            \n",
    "            \n",
    "        #if gaussian noise or laplace\n",
    "        m = torch.distributions.laplace.Laplace(torch.tensor([0.0]), torch.tensor([1/gau_rate]))\n",
    "        for item in range(batch):\n",
    "            for layer_idx in range(10):\n",
    "                #original_dy_dx[item][layer_idx] =  original_dy_dx[item][layer_idx] + torch.empty(original_dy_dx[item][layer_idx].size()).normal_(mean=0,std=1/gau_rate).to(device)\n",
    "                original_dy_dx[item][layer_idx] =  original_dy_dx[item][layer_idx] + torch.squeeze(m.sample(sample_shape=original_dy_dx[item][layer_idx].size()),dim=-1).to(device)\n",
    "                #break\n",
    "        ##if deep gradient compression\n",
    "        #print (original_dy_dx[0][0][0])\n",
    "        #for item in range(batch):\n",
    "        #    for layer_idx in range(10):\n",
    "        #        if layer_idx == 0:    \n",
    "        #            flat_dy_dx = torch.flatten(original_dy_dx[item][layer_idx])\n",
    "        #        else:\n",
    "        #            flat_dy_dx = torch.cat((flat_dy_dx,torch.flatten(original_dy_dx[item][layer_idx])),0)\n",
    "        #sorted_dy_dx = flat_dy_dx.abs().sort()\n",
    "        #size = np.asarray(list(flat_dy_dx.shape))\n",
    "        #thresh = sorted_dy_dx[0][int(size * dgc_rate/100.0)]\n",
    "        #print (size)\n",
    "        #print (int(size * dgc_rate/100.0))\n",
    "        #print (thresh)\n",
    "        #print (sorted_dy_dx[0][-1])\n",
    "        #for item in range(batch):\n",
    "        #    for layer_idx in range(10):\n",
    "        #        shape_tmp = original_dy_dx[item][layer_idx].size()\n",
    "        #        flat_dy_dx_prune = torch.flatten(original_dy_dx[item][layer_idx])\n",
    "        #        size_tmp = np.asarray(list(flat_dy_dx_prune.shape))\n",
    "        #        for m in range(int(size_tmp)):\n",
    "        #            if flat_dy_dx_prune[m].abs()<=thresh:\n",
    "        #                flat_dy_dx_prune[m] = 0\n",
    "        #        original_dy_dx[item][layer_idx] = flat_dy_dx_prune.view(shape_tmp)\n",
    "        #print (original_dy_dx[0][0][0])\n",
    "              \n",
    "\n",
    "        # generate dummy data and label\n",
    "        import time\n",
    "\n",
    "        #if iter_ % 10 ==0: \n",
    "        if iter_ == 1:\n",
    "            \n",
    "            #print ('epoch',epoch,'iter',iter_)\n",
    "            for item in range(1):\n",
    "                start = time.process_time()\n",
    "                for rd in range(1):\n",
    "\n",
    "                    torch.manual_seed(100*rd)\n",
    "\n",
    "                    pat_1 = torch.rand([3,16,16])\n",
    "                    pat_2 = torch.cat((pat_1,pat_1),dim=1)\n",
    "                    pat_4 = torch.cat((pat_2,pat_2),dim=2)\n",
    "                    dummy_data = torch.unsqueeze(pat_4,dim=0).to(device).requires_grad_(True)     \n",
    "\n",
    "                    dummy_unsqueeze=torch.unsqueeze(gt_onehot_label[item],dim=0)\n",
    "\n",
    "                    dummy_label = torch.randn(gt_onehot_label[item].size()).to(device).requires_grad_(True)\n",
    "                    label_pred = torch.argmin(torch.sum(original_dy_dx[item][-2], dim=-1), dim=-1).detach().reshape((1,)).requires_grad_(False)\n",
    "                    label_pred_onehot = label_to_onehot(label_pred, num_classes=3)\n",
    "\n",
    "                    plt.imshow(tt(dummy_data[0].cpu()))\n",
    "                    plt.title(\"Dummy data\")\n",
    "                    #plt.savefig(\"./random_seed/index_%s_rand_seed_%s_label_%s\"%(item,rd,torch.argmax(dummy_label, dim=-1).item()))\n",
    "\n",
    "                    plt.clf()\n",
    "                    #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "                    #print(\"stolen label is %d.\" % label_pred.item())\n",
    "\n",
    "                    #optimizer = torch.optim.LBFGS([dummy_data,dummy_label])\n",
    "                    optimizer = torch.optim.LBFGS([dummy_data,])\n",
    "\n",
    "\n",
    "                    history = []\n",
    "                    history_batch = []\n",
    "                    history_grad = []\n",
    "\n",
    "                    percept_dis = np.zeros(100)\n",
    "                    recover_dis = np.zeros(100)\n",
    "                    for iters in range(100):\n",
    "\n",
    "                        percept_dis[iters]=ssim(dummy_data,torch.unsqueeze(gt_data[item],dim=0),data_range=0).item()\n",
    "                        recover_dis[iters]=torch.dist(dummy_data,torch.unsqueeze(gt_data[item],dim=0),2).item()\n",
    "\n",
    "                        history.append(tt(dummy_data[0].cpu()))\n",
    "\n",
    "                        def closure():\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                            pred = net(dummy_data) \n",
    "                            dummy_onehot_label = F.softmax(dummy_label, dim=-1)\n",
    "                            #dummy_loss = criterion(pred, dummy_onehot_label) # TODO: fix the gt_label to dummy_label in both code and slides.\n",
    "\n",
    "                            #dummy_loss = criterion(pred, label_pred_onehot)\n",
    "                            dummy_loss = criterion(pred, label_pred)\n",
    "\n",
    "\n",
    "                            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "                            #dummy_dy_dp = torch.autograd.grad(dummy_loss, dummy_data, create_graph=True)\n",
    "                            #print (dummy_dy_dp[0].shape)\n",
    "\n",
    "                            grad_diff = 0\n",
    "                            grad_count = 0\n",
    "                            #count =0\n",
    "                            for gx, gy in zip(dummy_dy_dx, original_dy_dx[item]): # TODO: fix the variablas here\n",
    "\n",
    "                                #if iters==500 or iters== 1200:\n",
    "                                #    print (gx[0])\n",
    "                                #    print ('hahaha')\n",
    "                                #    print (gy[0])\n",
    "                                lasso = torch.norm(dummy_data,p=1)\n",
    "                                ridge = torch.norm(dummy_data,p=2)\n",
    "                                grad_diff += ((gx - gy) ** 2).sum() #+ 0.0*lasso +0.01*ridge\n",
    "\n",
    "\n",
    "                                grad_count += gx.nelement()\n",
    "\n",
    "                                #if count == 9:\n",
    "                                #    break\n",
    "                                #count=count+1\n",
    "                            # grad_diff = grad_diff / grad_count * 1000\n",
    "                            grad_diff.backward()\n",
    "                            #print (count)\n",
    "\n",
    "                            #print (dummy_dy_dx)\n",
    "                            #print (original_dy_dx)\n",
    "\n",
    "\n",
    "                            return grad_diff\n",
    "\n",
    "\n",
    "\n",
    "                        optimizer.step(closure)\n",
    "                        if iters % 5 == 0: \n",
    "                            current_loss = closure()\n",
    "                            #if iters == 0: \n",
    "                            #print (\"%.8f\" % current_loss.item())\n",
    "                            #print(iters, \"%.8f\" % current_loss.item())\n",
    "\n",
    "                    #     for bat in range(batch-1):\n",
    "                    #         history_batch.append(tt(dummy_data[bat].cpu()))\n",
    "\n",
    "                    #plt.figure(figsize=(30, 20))\n",
    "                    #for i in range(100):\n",
    "                    #    plt.subplot(10, 10, i + 1)\n",
    "                    #    plt.imshow(history[i * 5])\n",
    "                    #    plt.title(\"iter=%d\" % (i * 5))\n",
    "                    #    plt.axis('off')\n",
    "                    #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "\n",
    "                    #np.savetxt('./attack_image/lfw_ssim_idx_%s_laplace_%s'%(item,gau_rate),percept_dis,fmt=\"%4f\")\n",
    "                    #np.savetxt('./attack_image/lfw_mse_idx_%s_laplace_%s'%(item,gau_rate),recover_dis,fmt=\"%4f\")\n",
    "                    #plt.savefig(\"./attack_image/lfw_index_%s_laplace_%s\"%(item,gau_rate))\n",
    "\n",
    "                    #plt.clf()\n",
    "                    \n",
    "                    pinp = np.argmin(recover_dis)\n",
    "                    plt.imshow(history[pinp])\n",
    "                    plt.axis('off')\n",
    "                    \n",
    "                    #plt.figure(figsize=(15, 10))\n",
    "                    #for i in range(60):\n",
    "                    #    plt.subplot(6, 10, i + 1)\n",
    "                    #    plt.imshow(history[i * 14 ], cmap='gray')\n",
    "                    #    plt.title(\"iter=%d\" % (i*14))\n",
    "                    #    plt.axis('off')\n",
    "                    #plt.savefig(\"/content/sample_data\"%(item,gau_rate))\n",
    "                    \n",
    "                #duration = time.clock()-start\n",
    "                #print (\"Running time is %.4f.\" %(duration/10.0) )\n",
    "                #print (duration)\n",
    "                \n",
    "        \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "################################################### training when set to epoch\n",
    "\n",
    "#         #if epoch>=1:\n",
    "#         #if i==1:\n",
    "#             #break\n",
    "#         #print (iter_)\n",
    "#         inputs,label = data\n",
    "\n",
    "#         inputs,label =  Variable(inputs).to(device),Variable(label).to(device)\n",
    "\n",
    "#         optimizer_train.zero_grad()\n",
    "\n",
    "\n",
    "#         outputs_benign=net(inputs)\n",
    "#         #outputs_benign = F.softmax(outputs_benign, dim=-1)\n",
    "#         #print (outputs_benign[0])\n",
    "\n",
    "\n",
    "#         loss_benign =  criterion_train(outputs_benign,label)\n",
    "\n",
    "#         #print(\"loss computed\")\n",
    "#         loss_benign.backward()\n",
    "#         #print(\"loss BP\")\n",
    "#         optimizer_train.step()\n",
    "#         #sgd_update(net.parameters())\n",
    "\n",
    "#         #if i%2000==0:\n",
    "#         #print (loss_benign.item())\n",
    "#         #torch.save(net.state_dict(),'./LFW_net.pth')  \n",
    "\n",
    "#         #if  iter_%50==0:\n",
    "#         #    print ('attack',iter_)\n",
    "       \n",
    "        \n",
    "#         print ('fininshed training')\n",
    "#         break\n",
    "###############################   testing\n",
    "    \n",
    "#         total = len(y_test)\n",
    "#         acc =0.0\n",
    "#         for ct in range(total):\n",
    "#             testing_data = tt(testing[ct][0].cpu())\n",
    "#             testing_data1 = tp(testing_data).to(device)\n",
    "#             testing_data2 = testing_data1.view(1, *testing_data1.size())\n",
    "#             y_pred = net(testing_data2)\n",
    "#             predicted = torch.argmax(y_pred)\n",
    "\n",
    "#             if predicted == y_test[ct]:\n",
    "#                 acc=acc+1\n",
    "#         accuracy = acc / total\n",
    "#         print (accuracy)\n",
    "#         print ('fininshed testing')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_msssim import ssim\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    for iter_,data in enumerate(trainloader,1):\n",
    "   \n",
    "        ######### honest partipant #########\n",
    "        img_index = 30   #use img_index\n",
    "        dst_pil = tt(dst_tensor[img_index][0].cpu())   #use img_index\n",
    "\n",
    "        gt_data = transform(dst_pil).to(device)\n",
    "        gt_data = torch.unsqueeze(gt_data,0)\n",
    "\n",
    "        gt_label = dst_tensor[img_index][1].long().to(device) #use img_index\n",
    "        gt_label = gt_label.view(1, )\n",
    "        gt_onehot_label = label_to_onehot(gt_label, num_classes=3)\n",
    "\n",
    "        plt.imshow(dst_pil)\n",
    "        #plt.savefig(\"./original/index_%s_label_%s\"%(img_index,gt_label.item()))\n",
    "\n",
    "\n",
    "\n",
    "        batch =32  #\n",
    "        for bat in range(batch-1):\n",
    "            dst_pil = tt(dst_tensor[img_index+1+bat][0].cpu())   #use img_index\n",
    "            tmp = torch.unsqueeze(transform(dst_pil).to(device),0)\n",
    "            #print(tmp.shape)\n",
    "            gt_data = torch.cat((gt_data,tmp),0)\n",
    "\n",
    "            gt_label_tmp = dst_tensor[img_index+1+bat][1].long().to(device) #use img_index\n",
    "            gt_label_tmp = gt_label_tmp.view(1, )\n",
    "            gt_label = torch.cat((gt_label,gt_label_tmp),0)\n",
    "            gt_onehot_label = torch.cat((gt_onehot_label,label_to_onehot(gt_label_tmp, num_classes=3)),0)\n",
    "\n",
    "            plt.imshow(dst_pil)\n",
    "            #plt.savefig(\"./original/index_%s_label_%s\"%(bat+1,gt_label_tmp.item()))\n",
    "\n",
    "            #plt.title(\"Ground truth image\")\n",
    "            #print(\"GT label is %d.\" % gt_label.item(), \"\\nOnehot label is %d.\" % torch.argmax(gt_onehot_label, dim=-1).item())\n",
    "\n",
    "\n",
    "        gt_label = torch.reshape(gt_label,(-1,1))    \n",
    "        print (\"Image Shape = \",gt_data.shape)\n",
    "        print (\"Label shape = \",gt_label.shape)\n",
    "        print (\"Onehot Label shape = \",gt_onehot_label.shape)\n",
    "\n",
    "\n",
    "        # compute original gradient \n",
    "        dy_dx = []\n",
    "        original_dy_dx=[]\n",
    "        original_pred = []\n",
    "        for item in range(batch):\n",
    "            gt_data_single = torch.unsqueeze(gt_data[item],0)\n",
    "            out = net(gt_data_single)\n",
    "            #y = criterion(out, gt_onehot_label[item])\n",
    "            y = criterion(out, gt_label[item])\n",
    "            dy_dx = torch.autograd.grad(y, net.parameters(),retain_graph=True)\n",
    "            original_dy_dx_tmp = list((_.detach().clone() for _ in dy_dx))\n",
    "            original_dy_dx.append(original_dy_dx_tmp)\n",
    "            out_tmp = out.detach().clone()\n",
    "            original_pred.append(out_tmp)\n",
    "\n",
    "            #dy_dx.append(torch.autograd.grad(y, net.parameters()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # share the gradients with other clients\n",
    "        #original_dy_dx = list((_.detach().clone() for _ in dy_dx))\n",
    "\n",
    "\n",
    "        # generate dummy data and label\n",
    "        import time\n",
    "\n",
    "\n",
    "        for item in range(1):\n",
    "            start = time.process_time()\n",
    "            for rd in range(1):\n",
    "\n",
    "                torch.manual_seed(100*rd)\n",
    "        \n",
    "                pat_1 = torch.rand([3,16,16])\n",
    "                pat_2 = torch.cat((pat_1,pat_1),dim=1)\n",
    "                pat_4 = torch.cat((pat_2,pat_2),dim=2)\n",
    "                dummy_data = torch.unsqueeze(pat_4,dim=0).to(device).requires_grad_(True)     \n",
    "\n",
    "                dummy_unsqueeze=torch.unsqueeze(gt_onehot_label[item],dim=0)\n",
    "\n",
    "                dummy_label = torch.randn(dummy_unsqueeze.size()).to(device).requires_grad_(True)\n",
    "                label_pred=torch.argmin(torch.sum(original_dy_dx[item][-2], dim=-1), \n",
    "                                        dim=-1).detach().reshape((1,)).requires_grad_(False)\n",
    "                #print (original_dy_dx[item][-1].shape)\n",
    "                #print (original_dy_dx[item][-1].argmin())\n",
    "\n",
    "                #print (torch.sum(original_dy_dx[item][-2], dim=-1).argmin())\n",
    "\n",
    "                plt.imshow(tt(dummy_data[0].cpu()))\n",
    "                plt.title(\"Dummy data\")\n",
    "                #plt.savefig(\"./random_seed/index_%s_rand_seed_%s_label_%s\"%(item,rd,torch.argmax(dummy_label, dim=-1).item()))\n",
    "\n",
    "                plt.clf()\n",
    "                print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "                print(\"stolen label is %d.\" % label_pred.item())\n",
    "\n",
    "\n",
    "                #optimizer = torch.optim.LBFGS([dummy_data,dummy_label])\n",
    "                optimizer = torch.optim.LBFGS([dummy_data,])\n",
    "                #optimizer = torch.optim.AdamW([dummy_data,],lr=0.01)\n",
    "\n",
    "\n",
    "                history = []\n",
    "                history_batch = []\n",
    "                history_grad = []\n",
    "                \n",
    "                percept_dis = np.zeros(500)\n",
    "                recover_dis = np.zeros(500)\n",
    "                for iters in range(500):\n",
    "                    \n",
    "                    percept_dis[iters]=ssim(dummy_data,torch.unsqueeze(gt_data[item],dim=0),data_range=0).item()\n",
    "                    #recover_dis[iters]=torch.dist(dummy_data,torch.unsqueeze(gt_data[item],dim=0),2).item()\n",
    "                    recover_dis[iters]= F.mse_loss(dummy_data,torch.unsqueeze(gt_data[item],dim=0)).item()\n",
    "                    \n",
    "                    history.append(tt(dummy_data[0].cpu()))\n",
    "\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        pred = net(dummy_data) \n",
    "                        #dummy_onehot_label = F.softmax(dummy_label, dim=-1).long()\n",
    "\n",
    "                        #dummy_loss = criterion(pred, dummy_onehot_label) # TODO: fix the gt_label to dummy_label in both code and slides.\n",
    "                        #print (pred)\n",
    "                        #print (label_pred)\n",
    "\n",
    "                        dummy_loss = criterion(pred, label_pred)\n",
    "                        dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "                        #dummy_dy_dp = torch.autograd.grad(dummy_loss, dummy_data, create_graph=True)\n",
    "                        #print (dummy_dy_dp[0].shape)  \n",
    "\n",
    "                        grad_diff = 0\n",
    "                        grad_count = 0\n",
    "                        #count =0\n",
    "                        for gx, gy in zip(dummy_dy_dx, original_dy_dx[item]): # TODO: fix the variablas here\n",
    "\n",
    "                            #if iters==500 or iters== 1200:\n",
    "                            #print (gx[0])\n",
    "                            #    print ('hahaha')\n",
    "                            #print (gy[0])\n",
    "                            lasso = torch.norm(dummy_data,p=1)\n",
    "                            ridge = torch.norm(dummy_data,p=2)\n",
    "                            grad_diff += ((gx - gy) ** 2).sum() #+ 0.0*lasso +0.01*ridge \n",
    "\n",
    "                            #print (gx.shape)\n",
    "\n",
    "                            grad_count += gx.nelement()\n",
    "\n",
    "\n",
    "                            #if count == 9:\n",
    "                            #    break\n",
    "                            #count=count+1\n",
    "                        # grad_diff = grad_diff / grad_count * 1000\n",
    "\n",
    "                        #grad_diff += ((original_pred[item]-pred)**2).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        grad_diff.backward()\n",
    "                        #print (count)\n",
    "\n",
    "                        #print (dummy_dy_dx)\n",
    "                        #print (original_dy_dx)\n",
    "\n",
    "\n",
    "                        return grad_diff\n",
    "\n",
    "\n",
    "\n",
    "                    optimizer.step(closure)\n",
    "                    if iters % 5 == 0: \n",
    "                        current_loss = closure()\n",
    "                        #if iters == 0: \n",
    "                        #print (\"%.8f\" % current_loss.item())\n",
    "                        #print(iters, \"%.8f\" % current_loss.item())\n",
    "\n",
    "                #     for bat in range(batch-1):\n",
    "                #         history_batch.append(tt(dummy_data[bat].cpu()))\n",
    "\n",
    "                plt.figure(figsize=(30, 20))\n",
    "                for i in range(100):\n",
    "                    plt.subplot(10, 10, i + 1)\n",
    "                    plt.imshow(history[i * 5])\n",
    "                    plt.title(\"iter=%d\" % (i * 5))\n",
    "                    plt.axis('off')\n",
    "                #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "                \n",
    "                #np.savetxt('lfw_ssim_%s'%iter_,percept_dis,fmt=\"%4f\")\n",
    "                #np.savetxt('lfw_mse_%s'%iter_,recover_dis,fmt=\"%4f\")\n",
    "                #plt.savefig(\"./attack_image/index_%s_iter_%s_label_%s\"%(img_index,iter_,torch.argmax(dummy_label, dim=-1).item()))\n",
    "                \n",
    "                plt.clf()\n",
    "            duration = time.process_time()-start\n",
    "            #print (\"Running time is %.4f.\" %(duration/10.0) )\n",
    "            print (\"Duration = \", duration)\n",
    "            \n",
    "            \n",
    "            #if epoch>=1:\n",
    "        #if i==1:\n",
    "            #break\n",
    "        #print (iter_)\n",
    "        inputs,label = data\n",
    "\n",
    "        inputs,label =  Variable(inputs),Variable(label) \n",
    "\n",
    "        optimizer_train.zero_grad()\n",
    "\n",
    "\n",
    "        outputs_benign=net(inputs)\n",
    "        #outputs_benign = F.softmax(outputs_benign, dim=-1)\n",
    "        #print (outputs_benign[0])\n",
    "\n",
    "\n",
    "        loss_benign =  criterion_train(outputs_benign,label)\n",
    "\n",
    "        #print(\"loss computed\")\n",
    "        loss_benign.backward()\n",
    "        #print(\"loss BP\")\n",
    "        optimizer_train.step()\n",
    "\n",
    "        #if i%2000==0:\n",
    "        print (\"Loss Benign = \",loss_benign.item())\n",
    "        #torch.save(net.state_dict(),'./LFW_net.pth')  \n",
    "\n",
    "  \n",
    "        print ('fininshed training')\n",
    "        total = len(y_test)\n",
    "        acc =0.0\n",
    "        for ct in range(total):\n",
    "            testing_data = tt(testing[ct][0].cpu())\n",
    "            testing_data1 = transform(testing_data).to(device)\n",
    "            testing_data2 = testing_data1.view(1, *testing_data1.size())\n",
    "            y_pred = net(testing_data2)\n",
    "            predicted = torch.argmax(y_pred)\n",
    "\n",
    "            if predicted == y_test[ct]:\n",
    "                acc=acc+1\n",
    "        accuracy = acc / total\n",
    "        print (\"accuracy = \",accuracy)\n",
    "        print ('fininshed testing')\n",
    "        print(\"ssim_random2 = \", percept_dis)\n",
    "        print(\"mse_random2 = \", recover_dis)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(60):\n",
    "  plt.subplot(6, 10, i + 1)\n",
    "  plt.imshow(history[i * 5])\n",
    "  plt.title(\"iter=%d\" % (i * 5))\n",
    "  plt.axis('off')\n",
    "print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise level 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_msssim import ssim\n",
    "\n",
    "#dgc_rate_list = [20,30,40,50,70, 80,90]\n",
    "\n",
    "#for epoch in range(1):\n",
    "#for dgc_rate in dgc_rate_list:\n",
    "gau_rate_list = [400]\n",
    "for gau_rate in gau_rate_list:\n",
    "\n",
    "    \n",
    "    print('now starting',gau_rate)\n",
    "\n",
    "    for iter_,data in enumerate(trainloader,1):\n",
    "        \n",
    "        if iter_ != 1:\n",
    "            break\n",
    "   \n",
    "        ######### honest partipant #########\n",
    "        img_index = 30   #use img_index\n",
    "        dst_pil = tt(dst_tensor[img_index][0].cpu())   #use img_index\n",
    "\n",
    "        gt_data = transform(dst_pil).to(device)\n",
    "        gt_data = torch.unsqueeze(gt_data,0)\n",
    "\n",
    "        gt_label = dst_tensor[img_index][1].long().to(device) #use img_index\n",
    "        gt_label = gt_label.view(1, )\n",
    "        gt_onehot_label = label_to_onehot(gt_label, num_classes=3)\n",
    "\n",
    "        #plt.imshow(dst_pil)\n",
    "        #plt.axis('off')\n",
    "        #plt.savefig(\"sample_data_COVID_Xray\")\n",
    "\n",
    "\n",
    "\n",
    "        batch =2  #\n",
    "        for bat in range(batch-1):\n",
    "            dst_pil = tt(dst_tensor[img_index+1+bat][0].cpu())   #use img_index\n",
    "            tmp = torch.unsqueeze(transform(dst_pil).to(device),0)\n",
    "            #print(tmp.shape)\n",
    "            gt_data = torch.cat((gt_data,tmp),0)\n",
    "\n",
    "            gt_label_tmp = dst_tensor[img_index+1+bat][1].long().to(device) #use img_index\n",
    "            gt_label_tmp = gt_label_tmp.view(1, )\n",
    "            gt_label = torch.cat((gt_label,gt_label_tmp),0)\n",
    "            gt_onehot_label = torch.cat((gt_onehot_label,label_to_onehot(gt_label_tmp, num_classes=3)),0)\n",
    "\n",
    "            plt.imshow(dst_pil)\n",
    "            #plt.savefig(\"./original/index_%s_label_%s\"%(bat+1,gt_label_tmp.item()))\n",
    "\n",
    "            #plt.title(\"Ground truth image\")\n",
    "            #print(\"GT label is %d.\" % gt_label.item(), \"\\nOnehot label is %d.\" % torch.argmax(gt_onehot_label, dim=-1).item())\n",
    "\n",
    "\n",
    "        gt_label = torch.reshape(gt_label,(-1,1))    \n",
    "        #print (gt_data.shape)\n",
    "        #print (gt_label.shape)\n",
    "        #print (gt_onehot_label.shape)\n",
    "\n",
    "\n",
    "        # compute original gradient \n",
    "        dy_dx = []\n",
    "        original_dy_dx=[]\n",
    "        original_pred = []\n",
    "        for item in range(batch):\n",
    "            gt_data_single = torch.unsqueeze(gt_data[item],0)\n",
    "            out = net(gt_data_single)\n",
    "            #y = criterion(out, gt_onehot_label[item])\n",
    "            y = criterion(out, gt_label[item])\n",
    "            dy_dx = torch.autograd.grad(y, net.parameters(),retain_graph=True)\n",
    "            original_dy_dx_tmp = list((_.detach().clone() for _ in dy_dx))\n",
    "            original_dy_dx.append(original_dy_dx_tmp)\n",
    "            out_tmp = out.detach().clone()\n",
    "            original_pred.append(out_tmp)\n",
    "            \n",
    "            \n",
    "        #if gaussian noise or laplace\n",
    "        m = torch.distributions.laplace.Laplace(torch.tensor([0.0]), torch.tensor([1/gau_rate]))\n",
    "        for item in range(batch):\n",
    "            for layer_idx in range(10):\n",
    "                #original_dy_dx[item][layer_idx] =  original_dy_dx[item][layer_idx] + torch.empty(original_dy_dx[item][layer_idx].size()).normal_(mean=0,std=1/gau_rate).to(device)\n",
    "                original_dy_dx[item][layer_idx] =  original_dy_dx[item][layer_idx] + torch.squeeze(m.sample(sample_shape=original_dy_dx[item][layer_idx].size()),dim=-1).to(device)\n",
    "                #break\n",
    "        ##if deep gradient compression\n",
    "        #print (original_dy_dx[0][0][0])\n",
    "        #for item in range(batch):\n",
    "        #    for layer_idx in range(10):\n",
    "        #        if layer_idx == 0:    \n",
    "        #            flat_dy_dx = torch.flatten(original_dy_dx[item][layer_idx])\n",
    "        #        else:\n",
    "        #            flat_dy_dx = torch.cat((flat_dy_dx,torch.flatten(original_dy_dx[item][layer_idx])),0)\n",
    "        #sorted_dy_dx = flat_dy_dx.abs().sort()\n",
    "        #size = np.asarray(list(flat_dy_dx.shape))\n",
    "        #thresh = sorted_dy_dx[0][int(size * dgc_rate/100.0)]\n",
    "        #print (size)\n",
    "        #print (int(size * dgc_rate/100.0))\n",
    "        #print (thresh)\n",
    "        #print (sorted_dy_dx[0][-1])\n",
    "        #for item in range(batch):\n",
    "        #    for layer_idx in range(10):\n",
    "        #        shape_tmp = original_dy_dx[item][layer_idx].size()\n",
    "        #        flat_dy_dx_prune = torch.flatten(original_dy_dx[item][layer_idx])\n",
    "        #        size_tmp = np.asarray(list(flat_dy_dx_prune.shape))\n",
    "        #        for m in range(int(size_tmp)):\n",
    "        #            if flat_dy_dx_prune[m].abs()<=thresh:\n",
    "        #                flat_dy_dx_prune[m] = 0\n",
    "        #        original_dy_dx[item][layer_idx] = flat_dy_dx_prune.view(shape_tmp)\n",
    "        #print (original_dy_dx[0][0][0])\n",
    "              \n",
    "\n",
    "        # generate dummy data and label\n",
    "        import time\n",
    "\n",
    "        #if iter_ % 10 ==0: \n",
    "        if iter_ == 1:\n",
    "            \n",
    "            #print ('epoch',epoch,'iter',iter_)\n",
    "            for item in range(1):\n",
    "                start = time.process_time()\n",
    "                for rd in range(1):\n",
    "\n",
    "                    torch.manual_seed(100*rd)\n",
    "\n",
    "                    pat_1 = torch.rand([3,16,16])\n",
    "                    pat_2 = torch.cat((pat_1,pat_1),dim=1)\n",
    "                    pat_4 = torch.cat((pat_2,pat_2),dim=2)\n",
    "                    dummy_data = torch.unsqueeze(pat_4,dim=0).to(device).requires_grad_(True)     \n",
    "\n",
    "                    dummy_unsqueeze=torch.unsqueeze(gt_onehot_label[item],dim=0)\n",
    "\n",
    "                    dummy_label = torch.randn(gt_onehot_label[item].size()).to(device).requires_grad_(True)\n",
    "                    label_pred = torch.argmin(torch.sum(original_dy_dx[item][-2], dim=-1), dim=-1).detach().reshape((1,)).requires_grad_(False)\n",
    "                    label_pred_onehot = label_to_onehot(label_pred, num_classes=3)\n",
    "\n",
    "                    plt.imshow(tt(dummy_data[0].cpu()))\n",
    "                    plt.title(\"Dummy data\")\n",
    "                    #plt.savefig(\"./random_seed/index_%s_rand_seed_%s_label_%s\"%(item,rd,torch.argmax(dummy_label, dim=-1).item()))\n",
    "\n",
    "                    plt.clf()\n",
    "                    #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "                    #print(\"stolen label is %d.\" % label_pred.item())\n",
    "\n",
    "                    #optimizer = torch.optim.LBFGS([dummy_data,dummy_label])\n",
    "                    optimizer = torch.optim.LBFGS([dummy_data,])\n",
    "\n",
    "\n",
    "                    history = []\n",
    "                    history_batch = []\n",
    "                    history_grad = []\n",
    "\n",
    "                    percept_dis = np.zeros(100)\n",
    "                    recover_dis = np.zeros(100)\n",
    "                    for iters in range(100):\n",
    "\n",
    "                        percept_dis[iters]=ssim(dummy_data,torch.unsqueeze(gt_data[item],dim=0),data_range=0).item()\n",
    "                        recover_dis[iters]=torch.dist(dummy_data,torch.unsqueeze(gt_data[item],dim=0),2).item()\n",
    "\n",
    "                        history.append(tt(dummy_data[0].cpu()))\n",
    "\n",
    "                        def closure():\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                            pred = net(dummy_data) \n",
    "                            dummy_onehot_label = F.softmax(dummy_label, dim=-1)\n",
    "                            #dummy_loss = criterion(pred, dummy_onehot_label) # TODO: fix the gt_label to dummy_label in both code and slides.\n",
    "\n",
    "                            #dummy_loss = criterion(pred, label_pred_onehot)\n",
    "                            dummy_loss = criterion(pred, label_pred)\n",
    "\n",
    "\n",
    "                            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "                            #dummy_dy_dp = torch.autograd.grad(dummy_loss, dummy_data, create_graph=True)\n",
    "                            #print (dummy_dy_dp[0].shape)\n",
    "\n",
    "                            grad_diff = 0\n",
    "                            grad_count = 0\n",
    "                            #count =0\n",
    "                            for gx, gy in zip(dummy_dy_dx, original_dy_dx[item]): # TODO: fix the variablas here\n",
    "\n",
    "                                #if iters==500 or iters== 1200:\n",
    "                                #    print (gx[0])\n",
    "                                #    print ('hahaha')\n",
    "                                #    print (gy[0])\n",
    "                                lasso = torch.norm(dummy_data,p=1)\n",
    "                                ridge = torch.norm(dummy_data,p=2)\n",
    "                                grad_diff += ((gx - gy) ** 2).sum() #+ 0.0*lasso +0.01*ridge\n",
    "\n",
    "\n",
    "                                grad_count += gx.nelement()\n",
    "\n",
    "                                #if count == 9:\n",
    "                                #    break\n",
    "                                #count=count+1\n",
    "                            # grad_diff = grad_diff / grad_count * 1000\n",
    "                            grad_diff.backward()\n",
    "                            #print (count)\n",
    "\n",
    "                            #print (dummy_dy_dx)\n",
    "                            #print (original_dy_dx)\n",
    "\n",
    "\n",
    "                            return grad_diff\n",
    "\n",
    "\n",
    "\n",
    "                        optimizer.step(closure)\n",
    "                        if iters % 5 == 0: \n",
    "                            current_loss = closure()\n",
    "                            #if iters == 0: \n",
    "                            #print (\"%.8f\" % current_loss.item())\n",
    "                            #print(iters, \"%.8f\" % current_loss.item())\n",
    "\n",
    "                    #     for bat in range(batch-1):\n",
    "                    #         history_batch.append(tt(dummy_data[bat].cpu()))\n",
    "\n",
    "                    #plt.figure(figsize=(30, 20))\n",
    "                    #for i in range(100):\n",
    "                    #    plt.subplot(10, 10, i + 1)\n",
    "                    #    plt.imshow(history[i * 5])\n",
    "                    #    plt.title(\"iter=%d\" % (i * 5))\n",
    "                    #    plt.axis('off')\n",
    "                    #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "\n",
    "                    #np.savetxt('./attack_image/lfw_ssim_idx_%s_laplace_%s'%(item,gau_rate),percept_dis,fmt=\"%4f\")\n",
    "                    #np.savetxt('./attack_image/lfw_mse_idx_%s_laplace_%s'%(item,gau_rate),recover_dis,fmt=\"%4f\")\n",
    "                    #plt.savefig(\"./attack_image/lfw_index_%s_laplace_%s\"%(item,gau_rate))\n",
    "\n",
    "                    #plt.clf()\n",
    "                    \n",
    "                    pinp = np.argmin(recover_dis)\n",
    "                    plt.imshow(history[pinp])\n",
    "                    plt.axis('off')\n",
    "                    \n",
    "                    #plt.figure(figsize=(15, 10))\n",
    "                    #for i in range(60):\n",
    "                    #    plt.subplot(6, 10, i + 1)\n",
    "                    #    plt.imshow(history[i * 14 ], cmap='gray')\n",
    "                    #    plt.title(\"iter=%d\" % (i*14))\n",
    "                    #    plt.axis('off')\n",
    "                    #plt.savefig(\"/content/sample_data\"%(item,gau_rate))\n",
    "                    \n",
    "                #duration = time.clock()-start\n",
    "                #print (\"Running time is %.4f.\" %(duration/10.0) )\n",
    "                #print (duration)\n",
    "                \n",
    "        \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "################################################### training when set to epoch\n",
    "\n",
    "#         #if epoch>=1:\n",
    "#         #if i==1:\n",
    "#             #break\n",
    "#         #print (iter_)\n",
    "#         inputs,label = data\n",
    "\n",
    "#         inputs,label =  Variable(inputs).to(device),Variable(label).to(device)\n",
    "\n",
    "#         optimizer_train.zero_grad()\n",
    "\n",
    "\n",
    "#         outputs_benign=net(inputs)\n",
    "#         #outputs_benign = F.softmax(outputs_benign, dim=-1)\n",
    "#         #print (outputs_benign[0])\n",
    "\n",
    "\n",
    "#         loss_benign =  criterion_train(outputs_benign,label)\n",
    "\n",
    "#         #print(\"loss computed\")\n",
    "#         loss_benign.backward()\n",
    "#         #print(\"loss BP\")\n",
    "#         optimizer_train.step()\n",
    "#         #sgd_update(net.parameters())\n",
    "\n",
    "#         #if i%2000==0:\n",
    "#         #print (loss_benign.item())\n",
    "#         #torch.save(net.state_dict(),'./LFW_net.pth')  \n",
    "\n",
    "#         #if  iter_%50==0:\n",
    "#         #    print ('attack',iter_)\n",
    "       \n",
    "        \n",
    "#         print ('fininshed training')\n",
    "#         break\n",
    "###############################   testing\n",
    "    \n",
    "#         total = len(y_test)\n",
    "#         acc =0.0\n",
    "#         for ct in range(total):\n",
    "#             testing_data = tt(testing[ct][0].cpu())\n",
    "#             testing_data1 = tp(testing_data).to(device)\n",
    "#             testing_data2 = testing_data1.view(1, *testing_data1.size())\n",
    "#             y_pred = net(testing_data2)\n",
    "#             predicted = torch.argmax(y_pred)\n",
    "\n",
    "#             if predicted == y_test[ct]:\n",
    "#                 acc=acc+1\n",
    "#         accuracy = acc / total\n",
    "#         print (accuracy)\n",
    "#         print ('fininshed testing')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_msssim import ssim\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    for iter_,data in enumerate(trainloader,1):\n",
    "   \n",
    "        ######### honest partipant #########\n",
    "        img_index = 30   #use img_index\n",
    "        dst_pil = tt(dst_tensor[img_index][0].cpu())   #use img_index\n",
    "\n",
    "        gt_data = transform(dst_pil).to(device)\n",
    "        gt_data = torch.unsqueeze(gt_data,0)\n",
    "\n",
    "        gt_label = dst_tensor[img_index][1].long().to(device) #use img_index\n",
    "        gt_label = gt_label.view(1, )\n",
    "        gt_onehot_label = label_to_onehot(gt_label, num_classes=3)\n",
    "\n",
    "        plt.imshow(dst_pil)\n",
    "        #plt.savefig(\"./original/index_%s_label_%s\"%(img_index,gt_label.item()))\n",
    "\n",
    "\n",
    "\n",
    "        batch =32  #\n",
    "        for bat in range(batch-1):\n",
    "            dst_pil = tt(dst_tensor[img_index+1+bat][0].cpu())   #use img_index\n",
    "            tmp = torch.unsqueeze(transform(dst_pil).to(device),0)\n",
    "            #print(tmp.shape)\n",
    "            gt_data = torch.cat((gt_data,tmp),0)\n",
    "\n",
    "            gt_label_tmp = dst_tensor[img_index+1+bat][1].long().to(device) #use img_index\n",
    "            gt_label_tmp = gt_label_tmp.view(1, )\n",
    "            gt_label = torch.cat((gt_label,gt_label_tmp),0)\n",
    "            gt_onehot_label = torch.cat((gt_onehot_label,label_to_onehot(gt_label_tmp, num_classes=3)),0)\n",
    "\n",
    "            plt.imshow(dst_pil)\n",
    "            #plt.savefig(\"./original/index_%s_label_%s\"%(bat+1,gt_label_tmp.item()))\n",
    "\n",
    "            #plt.title(\"Ground truth image\")\n",
    "            #print(\"GT label is %d.\" % gt_label.item(), \"\\nOnehot label is %d.\" % torch.argmax(gt_onehot_label, dim=-1).item())\n",
    "\n",
    "\n",
    "        gt_label = torch.reshape(gt_label,(-1,1))    \n",
    "        print (\"Image Shape = \",gt_data.shape)\n",
    "        print (\"Label shape = \",gt_label.shape)\n",
    "        print (\"Onehot Label shape = \",gt_onehot_label.shape)\n",
    "\n",
    "\n",
    "        # compute original gradient \n",
    "        dy_dx = []\n",
    "        original_dy_dx=[]\n",
    "        original_pred = []\n",
    "        for item in range(batch):\n",
    "            gt_data_single = torch.unsqueeze(gt_data[item],0)\n",
    "            out = net(gt_data_single)\n",
    "            #y = criterion(out, gt_onehot_label[item])\n",
    "            y = criterion(out, gt_label[item])\n",
    "            dy_dx = torch.autograd.grad(y, net.parameters(),retain_graph=True)\n",
    "            original_dy_dx_tmp = list((_.detach().clone() for _ in dy_dx))\n",
    "            original_dy_dx.append(original_dy_dx_tmp)\n",
    "            out_tmp = out.detach().clone()\n",
    "            original_pred.append(out_tmp)\n",
    "\n",
    "            #dy_dx.append(torch.autograd.grad(y, net.parameters()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # share the gradients with other clients\n",
    "        #original_dy_dx = list((_.detach().clone() for _ in dy_dx))\n",
    "\n",
    "\n",
    "        # generate dummy data and label\n",
    "        import time\n",
    "\n",
    "\n",
    "        for item in range(1):\n",
    "            start = time.process_time()\n",
    "            for rd in range(1):\n",
    "\n",
    "                torch.manual_seed(100*rd)\n",
    "        \n",
    "                pat_1 = torch.rand([3,16,16])\n",
    "                pat_2 = torch.cat((pat_1,pat_1),dim=1)\n",
    "                pat_4 = torch.cat((pat_2,pat_2),dim=2)\n",
    "                dummy_data = torch.unsqueeze(pat_4,dim=0).to(device).requires_grad_(True)     \n",
    "\n",
    "                dummy_unsqueeze=torch.unsqueeze(gt_onehot_label[item],dim=0)\n",
    "\n",
    "                dummy_label = torch.randn(dummy_unsqueeze.size()).to(device).requires_grad_(True)\n",
    "                label_pred=torch.argmin(torch.sum(original_dy_dx[item][-2], dim=-1), \n",
    "                                        dim=-1).detach().reshape((1,)).requires_grad_(False)\n",
    "                #print (original_dy_dx[item][-1].shape)\n",
    "                #print (original_dy_dx[item][-1].argmin())\n",
    "\n",
    "                #print (torch.sum(original_dy_dx[item][-2], dim=-1).argmin())\n",
    "\n",
    "                plt.imshow(tt(dummy_data[0].cpu()))\n",
    "                plt.title(\"Dummy data\")\n",
    "                #plt.savefig(\"./random_seed/index_%s_rand_seed_%s_label_%s\"%(item,rd,torch.argmax(dummy_label, dim=-1).item()))\n",
    "\n",
    "                plt.clf()\n",
    "                print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "                print(\"stolen label is %d.\" % label_pred.item())\n",
    "\n",
    "\n",
    "                #optimizer = torch.optim.LBFGS([dummy_data,dummy_label])\n",
    "                optimizer = torch.optim.LBFGS([dummy_data,])\n",
    "                #optimizer = torch.optim.AdamW([dummy_data,],lr=0.01)\n",
    "\n",
    "\n",
    "                history = []\n",
    "                history_batch = []\n",
    "                history_grad = []\n",
    "                \n",
    "                percept_dis = np.zeros(500)\n",
    "                recover_dis = np.zeros(500)\n",
    "                for iters in range(500):\n",
    "                    \n",
    "                    percept_dis[iters]=ssim(dummy_data,torch.unsqueeze(gt_data[item],dim=0),data_range=0).item()\n",
    "                    #recover_dis[iters]=torch.dist(dummy_data,torch.unsqueeze(gt_data[item],dim=0),2).item()\n",
    "                    recover_dis[iters]= F.mse_loss(dummy_data,torch.unsqueeze(gt_data[item],dim=0)).item()\n",
    "                    \n",
    "                    history.append(tt(dummy_data[0].cpu()))\n",
    "\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        pred = net(dummy_data) \n",
    "                        #dummy_onehot_label = F.softmax(dummy_label, dim=-1).long()\n",
    "\n",
    "                        #dummy_loss = criterion(pred, dummy_onehot_label) # TODO: fix the gt_label to dummy_label in both code and slides.\n",
    "                        #print (pred)\n",
    "                        #print (label_pred)\n",
    "\n",
    "                        dummy_loss = criterion(pred, label_pred)\n",
    "                        dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "                        #dummy_dy_dp = torch.autograd.grad(dummy_loss, dummy_data, create_graph=True)\n",
    "                        #print (dummy_dy_dp[0].shape)  \n",
    "\n",
    "                        grad_diff = 0\n",
    "                        grad_count = 0\n",
    "                        #count =0\n",
    "                        for gx, gy in zip(dummy_dy_dx, original_dy_dx[item]): # TODO: fix the variablas here\n",
    "\n",
    "                            #if iters==500 or iters== 1200:\n",
    "                            #print (gx[0])\n",
    "                            #    print ('hahaha')\n",
    "                            #print (gy[0])\n",
    "                            lasso = torch.norm(dummy_data,p=1)\n",
    "                            ridge = torch.norm(dummy_data,p=2)\n",
    "                            grad_diff += ((gx - gy) ** 2).sum() #+ 0.0*lasso +0.01*ridge \n",
    "\n",
    "                            #print (gx.shape)\n",
    "\n",
    "                            grad_count += gx.nelement()\n",
    "\n",
    "\n",
    "                            #if count == 9:\n",
    "                            #    break\n",
    "                            #count=count+1\n",
    "                        # grad_diff = grad_diff / grad_count * 1000\n",
    "\n",
    "                        #grad_diff += ((original_pred[item]-pred)**2).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        grad_diff.backward()\n",
    "                        #print (count)\n",
    "\n",
    "                        #print (dummy_dy_dx)\n",
    "                        #print (original_dy_dx)\n",
    "\n",
    "\n",
    "                        return grad_diff\n",
    "\n",
    "\n",
    "\n",
    "                    optimizer.step(closure)\n",
    "                    if iters % 5 == 0: \n",
    "                        current_loss = closure()\n",
    "                        #if iters == 0: \n",
    "                        #print (\"%.8f\" % current_loss.item())\n",
    "                        #print(iters, \"%.8f\" % current_loss.item())\n",
    "\n",
    "                #     for bat in range(batch-1):\n",
    "                #         history_batch.append(tt(dummy_data[bat].cpu()))\n",
    "\n",
    "                plt.figure(figsize=(30, 20))\n",
    "                for i in range(100):\n",
    "                    plt.subplot(10, 10, i + 1)\n",
    "                    plt.imshow(history[i * 5])\n",
    "                    plt.title(\"iter=%d\" % (i * 5))\n",
    "                    plt.axis('off')\n",
    "                #print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
    "                \n",
    "                #np.savetxt('lfw_ssim_%s'%iter_,percept_dis,fmt=\"%4f\")\n",
    "                #np.savetxt('lfw_mse_%s'%iter_,recover_dis,fmt=\"%4f\")\n",
    "                #plt.savefig(\"./attack_image/index_%s_iter_%s_label_%s\"%(img_index,iter_,torch.argmax(dummy_label, dim=-1).item()))\n",
    "                \n",
    "                plt.clf()\n",
    "            duration = time.process_time()-start\n",
    "            #print (\"Running time is %.4f.\" %(duration/10.0) )\n",
    "            print (\"Duration = \", duration)\n",
    "            \n",
    "            \n",
    "            #if epoch>=1:\n",
    "        #if i==1:\n",
    "            #break\n",
    "        #print (iter_)\n",
    "        inputs,label = data\n",
    "\n",
    "        inputs,label =  Variable(inputs),Variable(label) \n",
    "\n",
    "        optimizer_train.zero_grad()\n",
    "\n",
    "\n",
    "        outputs_benign=net(inputs)\n",
    "        #outputs_benign = F.softmax(outputs_benign, dim=-1)\n",
    "        #print (outputs_benign[0])\n",
    "\n",
    "\n",
    "        loss_benign =  criterion_train(outputs_benign,label)\n",
    "\n",
    "        #print(\"loss computed\")\n",
    "        loss_benign.backward()\n",
    "        #print(\"loss BP\")\n",
    "        optimizer_train.step()\n",
    "\n",
    "        #if i%2000==0:\n",
    "        print (\"Loss Benign = \",loss_benign.item())\n",
    "        #torch.save(net.state_dict(),'./LFW_net.pth')  \n",
    "\n",
    "  \n",
    "        print ('fininshed training')\n",
    "        total = len(y_test)\n",
    "        acc =0.0\n",
    "        for ct in range(total):\n",
    "            testing_data = tt(testing[ct][0].cpu())\n",
    "            testing_data1 = transform(testing_data).to(device)\n",
    "            testing_data2 = testing_data1.view(1, *testing_data1.size())\n",
    "            y_pred = net(testing_data2)\n",
    "            predicted = torch.argmax(y_pred)\n",
    "\n",
    "            if predicted == y_test[ct]:\n",
    "                acc=acc+1\n",
    "        accuracy = acc / total\n",
    "        print (\"accuracy = \",accuracy)\n",
    "        print ('fininshed testing')\n",
    "        print(\"ssim_random2 = \", percept_dis)\n",
    "        print(\"mse_random2 = \", recover_dis)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(60):\n",
    "  plt.subplot(6, 10, i + 1)\n",
    "  plt.imshow(history[i * 5])\n",
    "  plt.title(\"iter=%d\" % (i * 5))\n",
    "  plt.axis('off')\n",
    "print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(history[295])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Load the two images (as PIL Images or tensors)\n",
    "img1 = X_train[21]\n",
    "img2 = history[295]\n",
    "\n",
    "# Convert the images to tensors and reshape to (batch_size, channels, height, width)\n",
    "img1_tensor = F.to_tensor(img1).unsqueeze(0)\n",
    "img2_tensor = F.to_tensor(img2).unsqueeze(0)\n",
    "\n",
    "# Calculate SSIM between the two images\n",
    "ssim_value = ssim(img1_tensor, img2_tensor, data_range=1.0, size_average=True)\n",
    "\n",
    "print(f\"SSIM between the two images: {ssim_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "mse = F.mse_loss(img1_tensor, img2_tensor)\n",
    "print(f\"SSIM: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "GPU_env",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
